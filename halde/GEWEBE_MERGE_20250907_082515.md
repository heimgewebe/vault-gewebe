# Gewebe-Merge

**Zeitpunkt:** 2025-09-07 08:25:15
**Quelle:** `/private/var/mobile/Containers/Data/Application/99D88FA7-793D-42DF-B05F-90CAB1F4353F/Documents/vault-gewebe/weltgewebe-programmierung/weltgewebe-repo`
**Dateien (inkludiert):** 39
**Gesamtgröße:** 99.37 KB

## 📁 Struktur

```
📁 weltgewebe-repo/
├── 📁 .codex/
    ├── 📄 maintenance.soft.sh (216.00 B)
    └── 📄 setup.sh (389.00 B)
├── 📁 .devcontainer/
    ├── 📄 devcontainer.json (523.00 B)
    ├── 📄 Dockerfile (354.00 B)
    ├── 📄 postCreate.sh (1.22 KB)
    └── 📄 postStart.sh (515.00 B)
├── 📁 .github/
    └── 📁 workflows/
        └── 📄 ci.yml (3.80 KB)
├── 📁 apps/
    └── 📁 api-elysia/
        ├── 📁 src/
            ├── 📄 metrics.ts (1.86 KB)
            └── 📄 server.ts (1.92 KB)
        ├── 📄 package.json (425.00 B)
        └── 📄 tsconfig.json (225.00 B)
├── 📁 docs/
    ├── 📄 allerersterStich.md (9.40 KB)
    ├── 📄 fahrplan.md (4.46 KB)
    ├── 📄 inhalt.md (9.47 KB)
    ├── 📄 webstack.md (5.40 KB)
    ├── 📄 wg.md (592.00 B)
    └── 📄 zusammenstellung.md (9.83 KB)
├── 📁 packages/
    ├── 📁 contracts/
        ├── 📁 src/
            └── 📄 index.ts (410.00 B)
        └── 📄 package.json (172.00 B)
    └── 📁 core/
        ├── 📁 src/
            └── 📄 index.ts (134.00 B)
        └── 📄 package.json (149.00 B)
├── 📁 scripts/
    ├── 📁 k6/
        └── 📄 smoke.js (608.00 B)
    ├── 📁 lighthouse/
        ├── 📄 budgets.json (218.00 B)
        └── 📄 lhci-mobile.sh (607.00 B)
    └── 📄 codex-run.sh (1.44 KB)
├── 📄 .api-dev.log (6.03 KB)
├── 📄 .git (140.00 B)
├── 📄 .gitignore (365.00 B)
├── 📄 bun.lock (11.55 KB)
├── 📄 e --continue (1.51 KB)
├── 📄 package.json (800.00 B)
├── 📄 README.md (142.00 B)
├── 📄 wg (8.73 KB)
├── 📄 wg.bak.1757191428 (3.13 KB)
├── 📄 wg.bak.1757191456 (3.13 KB)
├── 📄 wg.bak.1757191553 (3.13 KB)
├── 📄 wg.bak.1757191907 (3.06 KB)
├── 📄 wg.bak.1757192266 (1.38 KB)
└── 📄 wg.bak.1757192687 (2.11 KB)
```

## 🧾 Manifest

- .api-dev.log | md5=cf4e5686c97261bb15506e010cb619e7 | size=6177
- .codex/maintenance.soft.sh | md5=ad9f75a94c5a14da6a4e6c790e61c409 | size=216
- .codex/setup.sh | md5=bb3b4cad409def84ba20c6f4947e1025 | size=389
- .devcontainer/devcontainer.json | md5=c6d637e1b8d69cf7e77299e57dfd6762 | size=523
- .devcontainer/Dockerfile | md5=95e4572a7fcdce7dd1f57548d91ba614 | size=354
- .devcontainer/postCreate.sh | md5=6c0859d9cadb08392bf19ff316d9388d | size=1250
- .devcontainer/postStart.sh | md5=ef2195a7975e4fa9bd2eb8a96ea69d4d | size=515
- .git | md5=26d46d44c0e820afc45a04275a3ddbf3 | size=140
- .github/workflows/ci.yml | md5=cb5bbad5c5861932af99ad81d18a404a | size=3887
- .gitignore | md5=df44c2a0ced295f8d39f30f696a234aa | size=365
- apps/api-elysia/package.json | md5=41c81d1e0bc972254c03259464580e76 | size=425
- apps/api-elysia/src/metrics.ts | md5=dbfe0f93953eb239a87322d00b9f22ea | size=1907
- apps/api-elysia/src/server.ts | md5=750c49871d9d2c659de657c18dfeba80 | size=1971
- apps/api-elysia/tsconfig.json | md5=66abf84ce7d0d127e8daf9794f9797e9 | size=225
- bun.lock | md5=3e77219550dbe767916ad03a960a1fbd | size=11823
- docs/allerersterStich.md | md5=5dec8c48639fd9184f889b98ad8d130a | size=9629
- docs/fahrplan.md | md5=fda4984fcfb9c3f772b430f3e825aa4d | size=4564
- docs/inhalt.md | md5=7a908b6aac46294b34815d1a1f2fba75 | size=9696
- docs/webstack.md | md5=d022c17c71d32af68781069fab712724 | size=5532
- docs/wg.md | md5=28a912bf6aeb4d1e24b691d142ab325a | size=592
- docs/zusammenstellung.md | md5=6a195cd2a53856173081c518421d6e65 | size=10065
- e --continue | md5=477db6723663d90edf27ee63219bdc0e | size=1545
- package.json | md5=055032b79e56a2a61be0ad6c7feb1599 | size=800
- packages/contracts/package.json | md5=8a9ad7e36891b9e0eb0d4cb61fe35d90 | size=172
- packages/contracts/src/index.ts | md5=9982236363b211367f6d11cf05eaf7b2 | size=410
- packages/core/package.json | md5=3f4638a82ff7c33f9f77116ffa319719 | size=149
- packages/core/src/index.ts | md5=dddafb7b560628f80672d747b2803574 | size=134
- README.md | md5=0e191d4a1abf4c9b9b36c6b8ca286e7b | size=142
- scripts/codex-run.sh | md5=aa405aaf49be3e7cce108a0736d2b4f9 | size=1475
- scripts/k6/smoke.js | md5=af3175daef4e53356b4bf840883587fc | size=608
- scripts/lighthouse/budgets.json | md5=acd2a55266da226446749aa3b0ac91a7 | size=218
- scripts/lighthouse/lhci-mobile.sh | md5=b7622061d0310048c5e635393fc9ffb8 | size=607
- wg | md5=55b1f2c81b3834b7ead6905c78b33405 | size=8942
- wg.bak.1757191428 | md5=d59fb56350419c42b47bcef60f0daa97 | size=3203
- wg.bak.1757191456 | md5=d59fb56350419c42b47bcef60f0daa97 | size=3203
- wg.bak.1757191553 | md5=d59fb56350419c42b47bcef60f0daa97 | size=3203
- wg.bak.1757191907 | md5=9d7aac2bbbe894bd5ca43683baba6244 | size=3129
- wg.bak.1757192266 | md5=8e3d7357e16f24147bc394714a495e42 | size=1409
- wg.bak.1757192687 | md5=36fa6eabbe3555b26d45d499d7b3c627 | size=2160

## 📄 Dateiinhalte

### 📄 .api-dev.log

**Größe:** 6.03 KB

```
Usage: bun run [flags] <file or script>

Flags:
      --silent                        Don't print the script command
      --elide-lines=<val>             Number of lines of script output shown when using --filter (default: 10). Set to 0 to show all lines.
  -F, --filter=<val>                  Run a script in all workspace packages matching the pattern
  -b, --bun                           Force a script or package to use Bun's runtime instead of Node.js (via symlinking node)
      --shell=<val>                   Control the shell used for package.json scripts. Supports either 'bun' or 'system'
      --watch                         Automatically restart the process on file change
      --hot                           Enable auto reload in the Bun runtime, test runner, or bundler
      --no-clear-screen               Disable clearing the terminal screen on reload when --hot or --watch is enabled
      --smol                          Use less memory, but run garbage collection more often
  -r, --preload=<val>                 Import a module before other modules are loaded
      --require=<val>                 Alias of --preload, for Node.js compatibility
      --import=<val>                  Alias of --preload, for Node.js compatibility
      --inspect=<val>                 Activate Bun's debugger
      --inspect-wait=<val>            Activate Bun's debugger, wait for a connection before executing
      --inspect-brk=<val>             Activate Bun's debugger, set breakpoint on first line of code and wait
      --if-present                    Exit without an error if the entrypoint does not exist
      --no-install                    Disable auto install in the Bun runtime
      --install=<val>                 Configure auto-install behavior. One of "auto" (default, auto-installs when no node_modules), "fallback" (missing packages only), "force" (always).
  -i                                  Auto-install dependencies during execution. Equivalent to --install=fallback.
  -e, --eval=<val>                    Evaluate argument as a script
  -p, --print=<val>                   Evaluate argument as a script and print the result
      --prefer-offline                Skip staleness checks for packages in the Bun runtime and resolve from disk
      --prefer-latest                 Use the latest matching versions of packages in the Bun runtime, always checking npm
      --port=<val>                    Set the default port for Bun.serve
      --conditions=<val>              Pass custom conditions to resolve
      --fetch-preconnect=<val>        Preconnect to a URL while code is loading
      --max-http-header-size=<val>    Set the maximum size of HTTP headers in bytes. Default is 16KiB
      --dns-result-order=<val>        Set the default order of DNS lookup results. Valid orders: verbatim (default), ipv4first, ipv6first
      --expose-gc                     Expose gc() on the global object. Has no effect on Bun.gc().
      --no-deprecation                Suppress all reporting of the custom deprecation.
      --throw-deprecation             Determine whether or not deprecation warnings result in errors.
      --title=<val>                   Set the process title
      --zero-fill-buffers             Boolean to force Buffer.allocUnsafe(size) to be zero-filled.
      --redis-preconnect              Preconnect to $REDIS_URL at startup
      --sql-preconnect                Preconnect to PostgreSQL at startup
      --no-addons                     Throw an error if process.dlopen is called, and disable export condition "node-addons"
      --unhandled-rejections=<val>    One of "strict", "throw", "warn", "none", or "warn-with-error-code"
      --console-depth=<val>           Set the default depth for console.log object inspection (default: 2)
      --user-agent=<val>              Set the default User-Agent header for HTTP requests
      --main-fields=<val>             Main fields to lookup in package.json. Defaults to --target dependent
      --preserve-symlinks             Preserve symlinks when resolving files
      --preserve-symlinks-main        Preserve symlinks when resolving the main entry point
      --extension-order=<val>         Defaults to: .tsx,.ts,.jsx,.js,.json
      --tsconfig-override=<val>       Specify custom tsconfig.json. Default <d>$cwd<r>/tsconfig.json
  -d, --define=<val>                  Substitute K:V while parsing, e.g. --define process.env.NODE_ENV:"development". Values are parsed as JSON.
      --drop=<val>                    Remove function calls, e.g. --drop=console removes all console.* calls.
  -l, --loader=<val>                  Parse files with .ext:loader, e.g. --loader .js:jsx. Valid loaders: js, jsx, ts, tsx, json, toml, text, file, wasm, napi
      --no-macros                     Disable macros from being executed in the bundler, transpiler and runtime
      --jsx-factory=<val>             Changes the function called when compiling JSX elements using the classic JSX runtime
      --jsx-fragment=<val>            Changes the function called when compiling JSX fragments
      --jsx-import-source=<val>       Declares the module specifier to be used for importing the jsx and jsxs factory functions. Default: "react"
      --jsx-runtime=<val>             "automatic" (default) or "classic"
      --ignore-dce-annotations        Ignore tree-shaking annotations such as @__PURE__
      --env-file=<val>                Load environment variables from the specified file(s)
      --cwd=<val>                     Absolute path to resolve files & entry points from. This just changes the process' cwd.
  -c, --config=<val>                  Specify path to Bun config file. Default <d>$cwd<r>/bunfig.toml
  -h, --help                          Display this menu and exit

Examples:
  Run a JavaScript or TypeScript file
  bun run ./index.js
  bun run ./index.tsx

  Run a package.json script
  bun run dev
  bun run lint

Full documentation is available at https://bun.com/docs/cli/run

package.json scripts (3 found):
  $ bun run dev
    bun --hot src/server.ts

  $ bun run build
    bun build --target bun src/server.ts --outdir dist

  $ bun run check
    tsc --noEmit

```

### 📄 .codex/maintenance.soft.sh

**Größe:** 216.00 B

```bash
#!/usr/bin/env bash
set -euo pipefail
# Leichtes Warmup vor jeder Aufgabe (keine Netzoperationen).
export BUN_INSTALL="$HOME/.bun"
export PATH="$BUN_INSTALL/bin:$PATH"
echo "Codex maintenance: nothing to do (warm)."
```

### 📄 .codex/setup.sh

**Größe:** 389.00 B

```bash
#!/usr/bin/env bash
set -euo pipefail

# Bun installieren
if ! command -v bun >/dev/null 2>&1; then
  curl -fsSL https://bun.sh/install | bash
fi
export BUN_INSTALL="$HOME/.bun"
export PATH="$BUN_INSTALL/bin:$PATH"

# Node Tooling aktiv
corepack enable || true

# Abhängigkeiten
bun install --frozen-lockfile || true
bun install -C apps/web || true
bun install -C apps/api-elysia || true
```

### 📄 .devcontainer/devcontainer.json

**Größe:** 523.00 B

```json
{
  "name": "weltgewebe • bun 1.2.21",
  "build": { "dockerfile": "Dockerfile" },
  "remoteUser": "vscode",
  "workspaceFolder": "/workspaces/weltgewebe",
  "forwardPorts": [5173, 8787, 8080],
  "postCreateCommand": "bun install",
  "customizations": {
    "vscode": {
      "settings": {
        "terminal.integrated.defaultProfile.linux": "bash",
        "editor.formatOnSave": true
      },
      "extensions": [
        "oven.bun-vscode",
        "biomejs.biome",
        "dbaeumer.vscode-eslint"
      ]
    }
  }
}
```

### 📄 .devcontainer/Dockerfile

**Größe:** 354.00 B

```
FROM oven/bun:1.2.21-debian

# Tools, k6, git, curl etc. (schlank halten)
RUN apt-get update && apt-get install -y --no-install-recommends \
    git ca-certificates curl jq unzip \
 && rm -rf /var/lib/apt/lists/*

# Non-root user (codespace-konform)
ARG USERNAME=vscode
RUN useradd -m -s /bin/bash $USERNAME
USER $USERNAME
WORKDIR /workspaces/weltgewebe
```

### 📄 .devcontainer/postCreate.sh

**Größe:** 1.22 KB

```bash
#!/usr/bin/env bash
set -e  # bewusst kein -u/pipefail (Codespaces-tolerant)

echo "[wg] postCreate start"

# Bun-Pfad nur setzen, wenn existiert
if [ -d "$HOME/.bun/bin" ]; then
  export PATH="$HOME/.bun/bin:$PATH"
fi

# Dev-Zert ggf. erzeugen (leise)
mkdir -p .devcert
if [ ! -f .devcert/localhost.key ]; then
  if command -v openssl >/dev/null 2>&1; then
    openssl req -x509 -nodes -newkey rsa:2048 -days 3650 \
      -subj "/CN=localhost" \
      -keyout .devcert/localhost.key -out .devcert/localhost.crt >/dev/null 2>&1 || true
  fi
fi

# Dependencies nur wenn package.json vorhanden
if [ -f package.json ]; then
  if command -v bun >/dev/null 2>&1; then
    bun install --no-progress || true
  else
    echo "[wg] Hinweis: bun noch nicht im PATH – dependencies werden später installiert."
  fi
fi

# k6-Smoke-Stub (harmlos, idempotent)
mkdir -p scripts/k6
cat > scripts/k6/smoke.js << "EOF"
import http from "k6/http"; import { check } from "k6";
export const options = { vus: 1, iterations: 3 };
export default function () {
  const urls = ["http://localhost:3000/health","http://localhost:3000/metrics"];
  urls.forEach(u => check(http.get(u), { "status 2xx": r => r.status >=200 && r.status<300 }));
}
EOF

echo "[wg] postCreate done"
```

### 📄 .devcontainer/postStart.sh

**Größe:** 515.00 B

```bash
#!/usr/bin/env bash
set -e
if [ -d "$HOME/.bun/bin" ]; then
  export PATH="$HOME/.bun/bin:$PATH"
fi

# node_modules „self-heal“ nur versuchen, wenn bun da ist
if [ -d node_modules ] && command -v bun >/dev/null 2>&1; then
  bun install --no-progress || true
fi

echo -e "\n[wg] Devcontainer bereit. Beispiele:"
echo "  bun --version      # Bun prüfen"
echo "  bun run dev        # SvelteKit (falls vorhanden)"
echo "  bun run api        # Elysia API (falls vorhanden)"
echo "  bunx k6 run scripts/k6/smoke.js"
```

### 📄 .git

**Größe:** 140.00 B

```
gitdir: /private/var/mobile/Containers/Shared/AppGroup/7C18D54F-DE15-4549-B28E-92E4AF7801BC/GitFolders/C9CB5654-7FFB-46CF-A9B8-74EF2B60D29D/
```

### 📄 .github/workflows/ci.yml

**Größe:** 3.80 KB

```yaml
name: CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

permissions:
  contents: read

jobs:
  ci:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Bun
        uses: oven-sh/setup-bun@v2
        with:
          bun-version: "1.2.21"

      - name: Cache bun install
        uses: actions/cache@v4
        with:
          path: |
            ~/.bun/install/cache
            node_modules
            apps/**/node_modules
            packages/**/node_modules
          key: ${{ runner.os }}-bun-${{ hashFiles(**/bun.lockb) }}
          restore-keys: |
            ${{ runner.os }}-bun-

      - name: Install deps (root + workspaces)
        run: |
          bun install --frozen-lockfile
          [ -f apps/api-elysia/package.json ] && bun install -C apps/api-elysia || true
          [ -f apps/web/package.json ] && bun install -C apps/web || true

      - name: Lint (Biome, falls konfig vorhanden)
        run: |
          if [ -f biome.json ] || [ -f .biome.json ]; then
            bunx @biomejs/biome ci .
          else
            echo "No Biome config found – skipping lint."
          fi

      - name: Typecheck (API)
        run: bun --cwd apps/api-elysia run check

      - name: Unit Tests (bun) + Coverage (Artefakt)
        run: bun test --coverage || (echo "Tests failed"; exit 1)

      - name: Upload Coverage
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage
          path: coverage

      - name: Enforce Coverage >= 80%
        run: |
          set -e
          pct=""
          if [ -f coverage/lcov-report/index.html ]; then
            pct=$(grep -Eo "([0-9]{1,3}\.[0-9]|[0-9]{1,3})%" coverage/lcov-report/index.html | head -n1 | tr -d "%")
          fi
          if [ -z "$pct" ] && [ -f coverage/coverage-summary.json ]; then
            pct=$(node -e "const s=require(./coverage/coverage-summary.json); const t=s.total; const p=t&&t.lines&&t.lines.pct||0; console.log(p)")
          fi
          if [ -z "$pct" ] && [ -f coverage/coverage-final.json ]; then
            pct=$(node -e "
              const data=require(./coverage/coverage-final.json);
              let covered=0,total=0;
              for(const f of Object.values(data)){
                if(!f||!f.s) continue;
                for(const k in f.s){ total++; if(f.s[k]>0) covered++; }
              }
              const p = total? (covered*100/total):0;
              console.log(p.toFixed(2));
            ")
          fi
          if [ -z "$pct" ]; then echo "Coverage nicht ermittelbar – setze 0"; pct=0; fi
          echo "Coverage: ${pct}%"
          awk -v p="$pct" "BEGIN{ if (p+0 < 80) { print \"❌ Coverage < 80%\"; exit 1 } else { print \"✅ Coverage OK\" } }"

      - name: Build API
        run: bun --cwd apps/api-elysia run build

      - name: Start API (bg) & wait on /health
        run: |
          bun --cwd apps/api-elysia run dev >/tmp/api.log 2>&1 &
          for i in {1..30}; do
            if curl -fsS http://localhost:8787/health >/dev/null 2>&1; then
              echo "API up"; break
            fi
            sleep 1
          done
          curl -fsS http://localhost:8787/metrics | grep -q "wg_up" || { echo "metrics missing"; exit 1; }

      - name: k6 Smoke (/health,/metrics)
        env:
          API_BASE: http://localhost:8787
        run: bunx --yes k6@0.51.0 run scripts/k6/smoke.js

      - name: Axe (wartet auf apps/web)
        if: ${{ hashFiles(apps/web/package.json) !=  }}
        run: echo "TODO: axe-ci aktivieren, sobald apps/web existiert."

      - name: Stop API (immer)
        if: always()
        run: |
          pkill -f "bun .*apps/api-elysia" || true
          echo "--- API LOG ---"
          tail -n 200 /tmp/api.log || true
```

### 📄 .gitignore

**Größe:** 365.00 B

```
# OS / Editor
.DS_Store
Thumbs.db
*.swp
*~
*.log

# Backups / Temp
*.bak
*.tmp
*.temp
*.old
*.orig
*.rej

# Node / JS
node_modules/
.npm/
.pnpm-store/
.yarn/
dist/
.build/
coverage/
.vite/
.svelte-kit/

# Python
__pycache__/
*.py[cod]
.venv/
.uv/

# IDE
.idea/
.vscode/*
!.vscode/extensions.json
!.vscode/settings.json

# Misc
*.tgz
# wg temporär
wg.tmp*
wg.bak.*
```

### 📄 apps/api-elysia/package.json

**Größe:** 425.00 B

```json
{
  "name": "@welt/api-elysia",
  "version": "0.1.0",
  "type": "module",
  "scripts": {
    "dev": "bun --hot src/server.ts",
    "build": "bun build --target bun src/server.ts --outdir dist",
    "check": "tsc --noEmit"
  },
  "dependencies": {
    "elysia": "^1.3.21",
    "zod": "^3.23.8",
    "@welt/contracts": "workspace:*",
    "@welt/core": "workspace:*"
  },
  "devDependencies": {
    "typescript": "^5.6.2"
  }
}
```

### 📄 apps/api-elysia/src/metrics.ts

**Größe:** 1.86 KB

```typescript
type Labels = Record<string,string>;
const now = () => performance.now();

function line(name: string, value: number, labels?: Labels) {
  const lbl = labels && Object.keys(labels).length
    ? "{" + Object.entries(labels).map(([k,v]) => `${k}="${v}"`).join(",") + "}"
    : "";
  return `${name}${lbl} ${value}\n`;
}

export class Metrics {
  requestsTotal: Record<string, number> = {};
  duration: { buckets: number[], counts: Record<string, number[]> } = {
    buckets: [50,100,200,300,500,1000,2000], // ms
    counts: {}
  };
  up = 1;

  incRequest(route: string, method: string, status: number) {
    const key = `${route}|${method}|${status}`;
    this.requestsTotal[key] = (this.requestsTotal[key] ?? 0) + 1;
  }

  observeDuration(route: string, ms: number) {
    const key = route;
    if (!this.duration.counts[key]) this.duration.counts[key] = new Array(this.duration.buckets.length+1).fill(0);
    const idx = this.duration.buckets.findIndex(b => ms <= b);
    const bucketIndex = idx === -1 ? this.duration.buckets.length : idx;
    this.duration.counts[key][bucketIndex] += 1;
  }

  render() {
    let out = "";
    out += line("wg_up", this.up);
    for (const [k,v] of Object.entries(this.requestsTotal)) {
      const [route, method, status] = k.split("|");
      out += line("wg_requests_total", v, {route, method, status});
    }
    // histogram exposition
    for (const [route, arr] of Object.entries(this.duration.counts)) {
      let cum = 0;
      for (let i=0;i<arr.length;i++){
        cum += arr[i];
        const le = i < this.duration.buckets.length ? this.duration.buckets[i] : "+Inf";
        out += line("wg_request_duration_ms_bucket", cum, {route, le: String(le)});
      }
    }
    return out || "wg_up 1\n";
  }

  timer(route: string) {
    const start = now();
    return () => this.observeDuration(route, now()-start);
  }
}
export const metrics = new Metrics();
```

### 📄 apps/api-elysia/src/server.ts

**Größe:** 1.92 KB

```typescript
import { Elysia, t } from "elysia";
import { ok, err } from "@welt/core";
import { metrics } from "./metrics";

function reqId(): string {
  return crypto.randomUUID();
}

const app = new Elysia()
  // Request-ID + JSON-Log Middleware
  .onBeforeHandle(({ request, set }) => {
    const id = request.headers.get("x-request-id") || reqId();
    set.headers["x-request-id"] = id;
    (request as any)._rid = id;
  })
  .onAfterHandle(({ request, response, path }) => {
    const id = (request as any)._rid ?? "-";
    const method = request.method;
    const status = (response as any)?.status ?? 200;
    metrics.incRequest(path, method, status);
    console.log(JSON.stringify({
      ts: new Date().toISOString(),
      level: "info",
      request_id: id,
      method, path, status
    }));
  })

  // Health
  .get("/health", () => {
    const stop = metrics.timer("/health");
    stop();
    return { status: "ok", ts: new Date().toISOString() };
  })

  // Metrics (Prometheus-Textformat)
  .get("/metrics", () => metrics.render(), { detail: { type: "text/plain" } })

  // Echo (validiert, simples Beispiel)
  .post("/echo",
    ({ body }) => {
      const stop = metrics.timer("/echo");
      stop();
      return ok({ id: crypto.randomUUID(), message: body.message });
    },
    {
      body: t.Object({ message: t.String({ minLength: 1, maxLength: 200 }) })
    }
  )

  // Fehlerbeispiel (Fehlerformat + correlation_id)
  .get("/boom", () => {
    throw new Error("intentional");
  })
  .onError(({ error, request, set }) => {
    const id = (request as any)._rid ?? reqId();
    set.status = 500;
    console.error(JSON.stringify({
      ts: new Date().toISOString(),
      level: "error",
      request_id: id,
      error: { name: error.name, message: error.message }
    }));
    return err({ code: "INTERNAL_ERROR", message: "unexpected error", correlation_id: id });
  });

app.listen({ port: 8787 });
console.log(`API ready on http://localhost:8787`);
```

### 📄 apps/api-elysia/tsconfig.json

**Größe:** 225.00 B

```json
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "ES2022",
    "moduleResolution": "Bundler",
    "strict": true,
    "skipLibCheck": true,
    "types": ["bun"],
    "outDir": "dist"
  },
  "include": ["src"]
}
```

### 📄 bun.lock

**Größe:** 11.55 KB

```
{
  "lockfileVersion": 1,
  "workspaces": {
    "": {
      "name": "weltgewebe",
      "devDependencies": {
        "@biomejs/biome": "^1.9.4",
        "concurrently": "^9.0.1",
        "typescript": "^5.6.2",
        "zod": "^3.23.8",
      },
    },
    "apps/api-elysia": {
      "name": "@welt/api-elysia",
      "version": "0.1.0",
      "dependencies": {
        "@welt/contracts": "workspace:*",
        "@welt/core": "workspace:*",
        "elysia": "^1.3.21",
        "zod": "^3.23.8",
      },
      "devDependencies": {
        "typescript": "^5.6.2",
      },
    },
    "packages/contracts": {
      "name": "@welt/contracts",
      "version": "1.0.0",
      "devDependencies": {
        "typescript": "^5.6.2",
        "zod": "^3.23.8",
      },
    },
    "packages/core": {
      "name": "@welt/core",
      "version": "1.0.0",
      "devDependencies": {
        "typescript": "^5.6.2",
      },
    },
  },
  "packages": {
    "@biomejs/biome": ["@biomejs/biome@1.9.4", "", { "optionalDependencies": { "@biomejs/cli-darwin-arm64": "1.9.4", "@biomejs/cli-darwin-x64": "1.9.4", "@biomejs/cli-linux-arm64": "1.9.4", "@biomejs/cli-linux-arm64-musl": "1.9.4", "@biomejs/cli-linux-x64": "1.9.4", "@biomejs/cli-linux-x64-musl": "1.9.4", "@biomejs/cli-win32-arm64": "1.9.4", "@biomejs/cli-win32-x64": "1.9.4" }, "bin": { "biome": "bin/biome" } }, "sha512-1rkd7G70+o9KkTn5KLmDYXihGoTaIGO9PIIN2ZB7UJxFrWw04CZHPYiMRjYsaDvVV7hP1dYNRLxSANLaBFGpog=="],

    "@biomejs/cli-darwin-arm64": ["@biomejs/cli-darwin-arm64@1.9.4", "", { "os": "darwin", "cpu": "arm64" }, "sha512-bFBsPWrNvkdKrNCYeAp+xo2HecOGPAy9WyNyB/jKnnedgzl4W4Hb9ZMzYNbf8dMCGmUdSavlYHiR01QaYR58cw=="],

    "@biomejs/cli-darwin-x64": ["@biomejs/cli-darwin-x64@1.9.4", "", { "os": "darwin", "cpu": "x64" }, "sha512-ngYBh/+bEedqkSevPVhLP4QfVPCpb+4BBe2p7Xs32dBgs7rh9nY2AIYUL6BgLw1JVXV8GlpKmb/hNiuIxfPfZg=="],

    "@biomejs/cli-linux-arm64": ["@biomejs/cli-linux-arm64@1.9.4", "", { "os": "linux", "cpu": "arm64" }, "sha512-fJIW0+LYujdjUgJJuwesP4EjIBl/N/TcOX3IvIHJQNsAqvV2CHIogsmA94BPG6jZATS4Hi+xv4SkBBQSt1N4/g=="],

    "@biomejs/cli-linux-arm64-musl": ["@biomejs/cli-linux-arm64-musl@1.9.4", "", { "os": "linux", "cpu": "arm64" }, "sha512-v665Ct9WCRjGa8+kTr0CzApU0+XXtRgwmzIf1SeKSGAv+2scAlW6JR5PMFo6FzqqZ64Po79cKODKf3/AAmECqA=="],

    "@biomejs/cli-linux-x64": ["@biomejs/cli-linux-x64@1.9.4", "", { "os": "linux", "cpu": "x64" }, "sha512-lRCJv/Vi3Vlwmbd6K+oQ0KhLHMAysN8lXoCI7XeHlxaajk06u7G+UsFSO01NAs5iYuWKmVZjmiOzJ0OJmGsMwg=="],

    "@biomejs/cli-linux-x64-musl": ["@biomejs/cli-linux-x64-musl@1.9.4", "", { "os": "linux", "cpu": "x64" }, "sha512-gEhi/jSBhZ2m6wjV530Yy8+fNqG8PAinM3oV7CyO+6c3CEh16Eizm21uHVsyVBEB6RIM8JHIl6AGYCv6Q6Q9Tg=="],

    "@biomejs/cli-win32-arm64": ["@biomejs/cli-win32-arm64@1.9.4", "", { "os": "win32", "cpu": "arm64" }, "sha512-tlbhLk+WXZmgwoIKwHIHEBZUwxml7bRJgk0X2sPyNR3S93cdRq6XulAZRQJ17FYGGzWne0fgrXBKpl7l4M87Hg=="],

    "@biomejs/cli-win32-x64": ["@biomejs/cli-win32-x64@1.9.4", "", { "os": "win32", "cpu": "x64" }, "sha512-8Y5wMhVIPaWe6jw2H+KlEm4wP/f7EW3810ZLmDlrEEy5KvBsb9ECEfu/kMWD484ijfQ8+nIi0giMgu9g1UAuuA=="],

    "@borewit/text-codec": ["@borewit/text-codec@0.1.1", "", {}, "sha512-5L/uBxmjaCIX5h8Z+uu+kA9BQLkc/Wl06UGR5ajNRxu+/XjonB5i8JpgFMrPj3LXTCPA0pv8yxUvbUi+QthGGA=="],

    "@sinclair/typebox": ["@sinclair/typebox@0.34.41", "", {}, "sha512-6gS8pZzSXdyRHTIqoqSVknxolr1kzfy4/CeDnrzsVz8TTIWUbOBr6gnzOmTYJ3eXQNh4IYHIGi5aIL7sOZ2G/g=="],

    "@tokenizer/inflate": ["@tokenizer/inflate@0.2.7", "", { "dependencies": { "debug": "^4.4.0", "fflate": "^0.8.2", "token-types": "^6.0.0" } }, "sha512-MADQgmZT1eKjp06jpI2yozxaU9uVs4GzzgSL+uEq7bVcJ9V1ZXQkeGNql1fsSI0gMy1vhvNTNbUqrx+pZfJVmg=="],

    "@tokenizer/token": ["@tokenizer/token@0.3.0", "", {}, "sha512-OvjF+z51L3ov0OyAU0duzsYuvO01PH7x4t6DJx+guahgTnBHkhJdG7soQeTSFLWN3efnHyibZ4Z8l2EuWwJN3A=="],

    "@welt/api-elysia": ["@welt/api-elysia@workspace:apps/api-elysia"],

    "@welt/contracts": ["@welt/contracts@workspace:packages/contracts"],

    "@welt/core": ["@welt/core@workspace:packages/core"],

    "ansi-regex": ["ansi-regex@5.0.1", "", {}, "sha512-quJQXlTSUGL2LH9SUXo8VwsY4soanhgo6LNSm84E1LBcE8s3O0wpdiRzyR9z/ZZJMlMWv37qOOb9pdJlMUEKFQ=="],

    "ansi-styles": ["ansi-styles@4.3.0", "", { "dependencies": { "color-convert": "^2.0.1" } }, "sha512-zbB9rCJAT1rbjiVDb2hqKFHNYLxgtk8NURxZ3IZwD3F6NtxbXZQCnnSi1Lkx+IDohdPlFp222wVALIheZJQSEg=="],

    "chalk": ["chalk@4.1.2", "", { "dependencies": { "ansi-styles": "^4.1.0", "supports-color": "^7.1.0" } }, "sha512-oKnbhFyRIXpUuez8iBMmyEa4nbj4IOQyuhc/wy9kY7/WVPcwIO9VA668Pu8RkO7+0G76SLROeyw9CpQ061i4mA=="],

    "cliui": ["cliui@8.0.1", "", { "dependencies": { "string-width": "^4.2.0", "strip-ansi": "^6.0.1", "wrap-ansi": "^7.0.0" } }, "sha512-BSeNnyus75C4//NQ9gQt1/csTXyo/8Sb+afLAkzAptFuMsod9HFokGNudZpi/oQV73hnVK+sR+5PVRMd+Dr7YQ=="],

    "color-convert": ["color-convert@2.0.1", "", { "dependencies": { "color-name": "~1.1.4" } }, "sha512-RRECPsj7iu/xb5oKYcsFHSppFNnsj/52OVTRKb4zP5onXwVF3zVmmToNcOfGC+CRDpfK/U584fMg38ZHCaElKQ=="],

    "color-name": ["color-name@1.1.4", "", {}, "sha512-dOy+3AuW3a2wNbZHIuMZpTcgjGuLU/uBL/ubcZF9OXbDo8ff4O8yVp5Bf0efS8uEoYo5q4Fx7dY9OgQGXgAsQA=="],

    "concurrently": ["concurrently@9.2.1", "", { "dependencies": { "chalk": "4.1.2", "rxjs": "7.8.2", "shell-quote": "1.8.3", "supports-color": "8.1.1", "tree-kill": "1.2.2", "yargs": "17.7.2" }, "bin": { "conc": "dist/bin/concurrently.js", "concurrently": "dist/bin/concurrently.js" } }, "sha512-fsfrO0MxV64Znoy8/l1vVIjjHa29SZyyqPgQBwhiDcaW8wJc2W3XWVOGx4M3oJBnv/zdUZIIp1gDeS98GzP8Ng=="],

    "cookie": ["cookie@1.0.2", "", {}, "sha512-9Kr/j4O16ISv8zBBhJoi4bXOYNTkFLOqSL3UDB0njXxCXNezjeyVrJyGOWtgfs/q2km1gwBcfH8q1yEGoMYunA=="],

    "debug": ["debug@4.4.1", "", { "dependencies": { "ms": "^2.1.3" } }, "sha512-KcKCqiftBJcZr++7ykoDIEwSa3XWowTfNPo92BYxjXiyYEVrUQh2aLyhxBCwww+heortUFxEJYcRzosstTEBYQ=="],

    "elysia": ["elysia@1.3.21", "", { "dependencies": { "cookie": "^1.0.2", "exact-mirror": "0.1.6", "fast-decode-uri-component": "^1.0.1" }, "optionalDependencies": { "@sinclair/typebox": "^0.34.33", "openapi-types": "^12.1.3" }, "peerDependencies": { "file-type": ">= 20.0.0", "typescript": ">= 5.0.0" } }, "sha512-LLfDSoVA5fBoqKQfMJyzmHLkya8zMbEYwd7DS7v2iQB706mgzWg0gufXl58cFALErcvSayplrkDvjkmlYTkIZQ=="],

    "emoji-regex": ["emoji-regex@8.0.0", "", {}, "sha512-MSjYzcWNOA0ewAHpz0MxpYFvwg6yjy1NG3xteoqz644VCo/RPgnr1/GGt+ic3iJTzQ8Eu3TdM14SawnVUmGE6A=="],

    "escalade": ["escalade@3.2.0", "", {}, "sha512-WUj2qlxaQtO4g6Pq5c29GTcWGDyd8itL8zTlipgECz3JesAiiOKotd8JU6otB3PACgG6xkJUyVhboMS+bje/jA=="],

    "exact-mirror": ["exact-mirror@0.1.6", "", { "peerDependencies": { "@sinclair/typebox": "^0.34.15" }, "optionalPeers": ["@sinclair/typebox"] }, "sha512-EXGDixoDotCGrXCce63zmGHDA+3Id6PPkIwshBHuB10dwVc4YV4gfaYLuysHOxyURmwyt4UL186ann0oYa2CFQ=="],

    "fast-decode-uri-component": ["fast-decode-uri-component@1.0.1", "", {}, "sha512-WKgKWg5eUxvRZGwW8FvfbaH7AXSh2cL+3j5fMGzUMCxWBJ3dV3a7Wz8y2f/uQ0e3B6WmodD3oS54jTQ9HVTIIg=="],

    "fflate": ["fflate@0.8.2", "", {}, "sha512-cPJU47OaAoCbg0pBvzsgpTPhmhqI5eJjh/JIu8tPj5q+T7iLvW/JAYUqmE7KOB4R1ZyEhzBaIQpQpardBF5z8A=="],

    "file-type": ["file-type@21.0.0", "", { "dependencies": { "@tokenizer/inflate": "^0.2.7", "strtok3": "^10.2.2", "token-types": "^6.0.0", "uint8array-extras": "^1.4.0" } }, "sha512-ek5xNX2YBYlXhiUXui3D/BXa3LdqPmoLJ7rqEx2bKJ7EAUEfmXgW0Das7Dc6Nr9MvqaOnIqiPV0mZk/r/UpNAg=="],

    "get-caller-file": ["get-caller-file@2.0.5", "", {}, "sha512-DyFP3BM/3YHTQOCUL/w0OZHR0lpKeGrxotcHWcqNEdnltqFwXVfhEBQ94eIo34AfQpo0rGki4cyIiftY06h2Fg=="],

    "has-flag": ["has-flag@4.0.0", "", {}, "sha512-EykJT/Q1KjTWctppgIAgfSO0tKVuZUjhgMr17kqTumMl6Afv3EISleU7qZUzoXDFTAHTDC4NOoG/ZxU3EvlMPQ=="],

    "ieee754": ["ieee754@1.2.1", "", {}, "sha512-dcyqhDvX1C46lXZcVqCpK+FtMRQVdIMN6/Df5js2zouUsqG7I6sFxitIC+7KYK29KdXOLHdu9zL4sFnoVQnqaA=="],

    "is-fullwidth-code-point": ["is-fullwidth-code-point@3.0.0", "", {}, "sha512-zymm5+u+sCsSWyD9qNaejV3DFvhCKclKdizYaJUuHA83RLjb7nSuGnddCHGv0hk+KY7BMAlsWeK4Ueg6EV6XQg=="],

    "ms": ["ms@2.1.3", "", {}, "sha512-6FlzubTLZG3J2a/NVCAleEhjzq5oxgHyaCU9yYXvcLsvoVaHJq/s5xXI6/XXP6tz7R9xAOtHnSO/tXtF3WRTlA=="],

    "openapi-types": ["openapi-types@12.1.3", "", {}, "sha512-N4YtSYJqghVu4iek2ZUvcN/0aqH1kRDuNqzcycDxhOUpg7GdvLa2F3DgS6yBNhInhv2r/6I0Flkn7CqL8+nIcw=="],

    "require-directory": ["require-directory@2.1.1", "", {}, "sha512-fGxEI7+wsG9xrvdjsrlmL22OMTTiHRwAMroiEeMgq8gzoLC/PQr7RsRDSTLUg/bZAZtF+TVIkHc6/4RIKrui+Q=="],

    "rxjs": ["rxjs@7.8.2", "", { "dependencies": { "tslib": "^2.1.0" } }, "sha512-dhKf903U/PQZY6boNNtAGdWbG85WAbjT/1xYoZIC7FAY0yWapOBQVsVrDl58W86//e1VpMNBtRV4MaXfdMySFA=="],

    "shell-quote": ["shell-quote@1.8.3", "", {}, "sha512-ObmnIF4hXNg1BqhnHmgbDETF8dLPCggZWBjkQfhZpbszZnYur5DUljTcCHii5LC3J5E0yeO/1LIMyH+UvHQgyw=="],

    "string-width": ["string-width@4.2.3", "", { "dependencies": { "emoji-regex": "^8.0.0", "is-fullwidth-code-point": "^3.0.0", "strip-ansi": "^6.0.1" } }, "sha512-wKyQRQpjJ0sIp62ErSZdGsjMJWsap5oRNihHhu6G7JVO/9jIB6UyevL+tXuOqrng8j/cxKTWyWUwvSTriiZz/g=="],

    "strip-ansi": ["strip-ansi@6.0.1", "", { "dependencies": { "ansi-regex": "^5.0.1" } }, "sha512-Y38VPSHcqkFrCpFnQ9vuSXmquuv5oXOKpGeT6aGrr3o3Gc9AlVa6JBfUSOCnbxGGZF+/0ooI7KrPuUSztUdU5A=="],

    "strtok3": ["strtok3@10.3.4", "", { "dependencies": { "@tokenizer/token": "^0.3.0" } }, "sha512-KIy5nylvC5le1OdaaoCJ07L+8iQzJHGH6pWDuzS+d07Cu7n1MZ2x26P8ZKIWfbK02+XIL8Mp4RkWeqdUCrDMfg=="],

    "supports-color": ["supports-color@8.1.1", "", { "dependencies": { "has-flag": "^4.0.0" } }, "sha512-MpUEN2OodtUzxvKQl72cUF7RQ5EiHsGvSsVG0ia9c5RbWGL2CI4C7EpPS8UTBIplnlzZiNuV56w+FuNxy3ty2Q=="],

    "token-types": ["token-types@6.1.1", "", { "dependencies": { "@borewit/text-codec": "^0.1.0", "@tokenizer/token": "^0.3.0", "ieee754": "^1.2.1" } }, "sha512-kh9LVIWH5CnL63Ipf0jhlBIy0UsrMj/NJDfpsy1SqOXlLKEVyXXYrnFxFT1yOOYVGBSApeVnjPw/sBz5BfEjAQ=="],

    "tree-kill": ["tree-kill@1.2.2", "", { "bin": { "tree-kill": "cli.js" } }, "sha512-L0Orpi8qGpRG//Nd+H90vFB+3iHnue1zSSGmNOOCh1GLJ7rUKVwV2HvijphGQS2UmhUZewS9VgvxYIdgr+fG1A=="],

    "tslib": ["tslib@2.8.1", "", {}, "sha512-oJFu94HQb+KVduSUQL7wnpmqnfmLsOA/nAh6b6EH0wCEoK0/mPeXU6c3wKDV83MkOuHPRHtSXKKU99IBazS/2w=="],

    "typescript": ["typescript@5.9.2", "", { "bin": { "tsc": "bin/tsc", "tsserver": "bin/tsserver" } }, "sha512-CWBzXQrc/qOkhidw1OzBTQuYRbfyxDXJMVJ1XNwUHGROVmuaeiEm3OslpZ1RV96d7SKKjZKrSJu3+t/xlw3R9A=="],

    "uint8array-extras": ["uint8array-extras@1.5.0", "", {}, "sha512-rvKSBiC5zqCCiDZ9kAOszZcDvdAHwwIKJG33Ykj43OKcWsnmcBRL09YTU4nOeHZ8Y2a7l1MgTd08SBe9A8Qj6A=="],

    "wrap-ansi": ["wrap-ansi@7.0.0", "", { "dependencies": { "ansi-styles": "^4.0.0", "string-width": "^4.1.0", "strip-ansi": "^6.0.0" } }, "sha512-YVGIj2kamLSTxw6NsZjoBxfSwsn0ycdesmc4p+Q21c5zPuZ1pl+NfxVdxPtdHvmNVOQ6XSYG4AUtyt/Fi7D16Q=="],

    "y18n": ["y18n@5.0.8", "", {}, "sha512-0pfFzegeDWJHJIAmTLRP2DwHjdF5s7jo9tuztdQxAhINCdvS+3nGINqPd00AphqJR/0LhANUS6/+7SCb98YOfA=="],

    "yargs": ["yargs@17.7.2", "", { "dependencies": { "cliui": "^8.0.1", "escalade": "^3.1.1", "get-caller-file": "^2.0.5", "require-directory": "^2.1.1", "string-width": "^4.2.3", "y18n": "^5.0.5", "yargs-parser": "^21.1.1" } }, "sha512-7dSzzRQ++CKnNI/krKnYRV7JKKPUXMEh61soaHKg9mrWEhzFWhFnxPxGl+69cD1Ou63C13NUPCnmIcrvqCuM6w=="],

    "yargs-parser": ["yargs-parser@21.1.1", "", {}, "sha512-tVpsJW7DdjecAiFpbIB1e3qxIQsE6NoPc5/eTdrbbIC4h0LVsWhnoa3g+m2HclBIujHzsxZ4VJVA+GUuc2/LBw=="],

    "zod": ["zod@3.25.76", "", {}, "sha512-gzUt/qt81nXsFGKIFcC3YnfEAx5NkunCfnDlvuBSSFS02bcXu4Lmea0AFIUwbLWxWPx3d9p8S5QoaujKcNQxcQ=="],

    "chalk/supports-color": ["supports-color@7.2.0", "", { "dependencies": { "has-flag": "^4.0.0" } }, "sha512-qpCAvRl9stuOHveKsn7HncJRvv501qIacKzQlO/+Lwxc9+0q2wLyv4Dfvt80/DPn2pqOBsJdDiogXGR9+OvwRw=="],
  }
}
```

### 📄 docs/allerersterStich.md

**Größe:** 9.40 KB

```markdown
Alles klar—ich arbeite Phase 0 – Bootstrap & Guardrails jetzt aus. Fokus: präzise Deliverables, harte Kriterien, klarer Ablauf. Keine Magie, kein Overhead.

∴subtext: Wir gießen die Leitplanken in Beton, bevor irgendetwas schnell fährt.

Phase 0 – Bootstrap & Guardrails (ausgearbeitet)

Ziel

Ein arbeitsfähiges Monorepo mit den richtigen Schutzgeländern: reproduzierbar, messbar, scheitert früh, wenn etwas wackelt. Danach kann jede weitere Phase parallelisiert werden.

Ergebnisobjekte (müssen am Ende existieren)
	1.	Repo-Skelett
apps/ (Web, Programmierschnittstelle), packages/ (Verträge, Kern), infra/ (Zusammenspiel, Metriken, Protokolle), scripts/, docs/ (diese Blaupausen).
	2.	Programmierschnittstelle-Stub (Bun + Elysia)
Endpunkte:
	•	GET /health → {status:"ok", ts:"..."}
	•	GET /metrics → Textformat, mindestens wg_up 1 und Latenz-Histogramm
Eigenschaften:
	•	Anfrage-Kennung pro Aufruf (Request-ID in Protokoll und Antwort-Header)
	•	Protokolle im JSON-Format (eine Zeile pro Ereignis)
	•	Fehler-Antworten strikt schematisiert (Schlüssel: error.code, error.message, correlation_id)
	3.	Kanten-Absicherung (Caddy als Eingang)
	•	Strenge Inhaltsrichtlinie (Content Security Policy) inkl. Quellen der Karte
	•	„Hypertext Transfer Protocol“ Version 3 aktiviert, „Strict Transport Security“, „Referrer Policy“, „Cross-Origin Opener Policy“
	•	Rückwärtsverteiler zur Programmierschnittstelle; später zur Web-App
	4.	Leistungs- und Qualitätsbudgets (verbindlich)
	•	Start-JavaScript ≤ 70 Kilobyte
	•	„Largest Contentful Paint“ ≤ 1,8 Sekunden (Mobilnetz)
	•	Programmierschnittstelle p95 ≤ 150 Millisekunden
	•	Datenbank p95 ≤ 50 Millisekunden (hier nur Platzhalter-Metrik, echte Werte ab Phase 1)
	•	Ereignisverzug der Auslagerung (Outbox) < 1 Sekunde (ab Phase 1 relevant)
	5.	Kontinuierliche Lieferkette (GitHub Actions)
Pipeline (Reihenfolge ist verbindlich):
	1.	Lint (Biome)
	2.	Typprüfung (TypeScript)
	3.	Einheiten-Prüfungen (Bun-Test) mit Abdeckungsgrenze ≥ 80 Prozent
	4.	Erstellung (Build)
	5.	Kurzprüfung mit K6 (Gesundheit und Metriken)
	6.	Barrierefreiheit (Axe-Prüfung)
	7.	Sicherheits-Scan (Semgrep-Regeln: hoch-kritische Befunde blockieren)
	8.	Paketprüfung (npm-Audit in moderater Stufe)
	9.	Ausstiegshaken Hono: nur Kompilierung der kleinsten Kanten-Routen als Zukunftspfad
	6.	Beobachtbarkeit – Grundstock
	•	Prometheus holt die Metriken der Programmierschnittstelle
	•	Grafana-Dashboard-Gerüst (Rate, Anteil Fehler, Dauer p50/p90/p95)
	•	Protokolle in Loki (Retention vier Wochen in Entwicklung)
	7.	Versions-Pflege
	•	„Pin-Überprüfung“ (monatlicher Arbeitsablauf): erzeugt ein Thema mit Änderungen der Versionen und einem Verweis auf die Veröffentlichung

⸻

Arbeiten (in dieser Reihenfolge)

A) Monorepo anlegen
	•	package.json im Wurzelverzeichnis: Arbeitsbereiche apps/*, packages/*; „Bun“ als Paketverwalter eingetragen; „Node“ als Werkzeugspur ab Version 20 (nur für Werkzeuge).
	•	.gitignore und README mit Kurzbeschreibung und Projektzielen.

B) Gemeinsame Pakete
	•	packages/contracts: Zentrale Verträge (Zod-Schemata) mit Strikter Semantik-Versionierung.
	•	Regel: Brüche in den Verträgen sind verboten, solange apps/web der Hauptzweig ist; der Vergleich der Verträge ist ein Tor in der Lieferkette.
	•	packages/core: Rahmenfreie Hilfsfunktionen (keine Abhängigkeiten von einem Rahmen).

C) Programmierschnittstelle-Stub (Elysia)
	•	Endpunkte GET /health, GET /metrics, POST /echo (prüft Anforderung anhand des Vertrages und gibt sie zurück).
	•	Mittelware (Middleware):
	•	Anfrage-Kennung erzeugen (wenn fehlt) und in X-Request-ID setzen
	•	Protokolle als einzelne Zeilen im JSON-Format mit Zeitstempel in Coordinated Universal Time, request_id, method, path, status, duration_ms
	•	Einheitliches Fehlerformat (siehe oben)

D) Eingang (Caddy)
	•	Ein Site-Block für Entwicklung:
	•	Kompression (zstd, gzip)
	•	Inhaltsrichtlinie:
	•	Standard nur eigene Herkunft
	•	Skripte nur eigene Herkunft
	•	Stile eigene Herkunft und „unsafe-inline“ (bis zum Ausräumen in Phase 4)
	•	Bilder eigene Herkunft und Daten-Uniform Resource Identifier
	•	Verbindungen: eigene Herkunft und Programmierschnittstelle (/api)
	•	„Hypertext Transfer Protocol“ Version 3 aktiv
	•	„Strict Transport Security“ mit einjähriger Dauer
	•	„Referrer Policy: strict-origin-when-cross-origin“
	•	„Cross-Origin Opener Policy: same-origin“

E) Beobachtbarkeit – Grundstock
	•	Programmierschnittstelle liefert Metriken:
	•	Zähler wg_requests_total{route,method,status}
	•	Histogramm wg_request_duration_ms_bucket
	•	Einfacher Zähler wg_up
	•	Prometheus-Konfiguration: erfasst api:8787/metrics.
	•	Grafana: ein Start-Dashboard „Weltgewebe – Programmierschnittstelle (Entwicklung)“ mit drei Panels: Rate, Anteil Fehler, Dauer p95.
	•	Loki: einfache lokale Konfiguration mit Dateispeicher; Protokollformat einheitlich (eine Zeile, JSON).

F) Lieferkette (GitHub Actions)
	•	Arbeitsablauf „kontinuierliche Integration“ mit folgenden Toren (alle blockierend):
	•	Lint: Biome auf dem gesamten Projekt
	•	Typprüfung: TypeScript für Web und Programmierschnittstelle
	•	Einheiten-Prüfungen: Bun-Test, Bericht der Abdeckung; Tor: Abdeckung ≥ 80 Prozent
	•	Erstellung: Web und Programmierschnittstelle
	•	Kurzprüfung K6: zehn Sekunden Last, Ziel: p95 < 300 Millisekunden für /health und /metrics
	•	Barrierefreiheit: Axe-Prüfung des Startdokuments (später echte Seiten); Tor: keine schweren Verstöße
	•	Sicherheits-Scan: Semgrep; Tor: keine hoch-kritischen Treffer
	•	Paketprüfung: Paket-Audit mit Mäßigung (Produktion), Tor: blockiert nur bei hoher Kritikalität
	•	Ausstiegshaken Hono: Kompilierung der minimalen Kanten-Routen (nur Typsicherheit)
	•	Arbeitsablauf „Pins-Überprüfung“ (monatlich):
	•	Erzeugt ein Thema „Versions-Überprüfung “ mit Auflistung der aktuellen Abhängigkeiten und Hinweisen auf Veränderungen.

G) Qualitäts- und Leistungsbudgets als Code
	•	In scripts/ drei Dateien:
	1.	Lighthouse-Budgets (Mobil): Start-JavaScript ≤ 70 Kilobyte, „Largest Contentful Paint“ ≤ 1,8 Sekunden, „Cumulative Layout Shift“ ≤ 0,1
	2.	K6-Kurzprüfung für /health und /metrics mit Tor p95 < 300 Millisekunden
	3.	Grafana-Alarmregeln: Anteil Fehler > 1 Prozent für fünf Minuten → Warnung

H) Dokumentation
	•	docs/webstack.md (liegt bereits)
	•	docs/fahrplan.md (dieser Plan, Phase 0 ausgearbeitet)
	•	docs/entscheidungslogbuch.md (kurzer Eintrag: warum Bun, warum Elysia, warum Caddy)

⸻

Definition of Done (abzuhaken, sonst keine Phase 1)
	•	curl /health gibt 200 mit Status „ok“.
	•	curl /metrics enthält wg_up 1 und mindestens ein Histogramm.
	•	Protokolle der Programmierschnittstelle sind eine Zeile pro Ereignis im JSON-Format und enthalten Anfrage-Kennung und Dauer in Millisekunden.
	•	Caddy setzt „Strict Transport Security“, Inhaltsrichtlinie und „Hypertext Transfer Protocol“ Version 3 ist eingeschaltet.
	•	Lighthouse (Mobil) zeigt Start-JavaScript ≤ 70 Kilobyte und „Largest Contentful Paint“ ≤ 1,8 Sekunden (Platzhalterseite reicht, aber Messung ist Teil der Lieferkette).
	•	Lieferkette ist grün und bricht bei Verstößen der Tore ab.
	•	Prometheus kann die Programmierschnittstelle messen; Grafana zeigt zumindest Rate, Anteil Fehler, Dauer p95.
	•	Pins-Überprüfung erzeugt einen Entwurf eines Themas mit dem aktuellen Monat.

⸻

Prüfideen (schnelle manuelle Kontrollen)
	•	Fehlerpfad: Erzeuge bewusst einen Vertragsfehler bei POST /echo, prüfe, ob Fehlerantworten schematisch sind und die Anfrage-Kennung tragen.
	•	Latenz: Sende 2000 Zugriffe auf /health über zehn Sekunden (K6), überprüfe p95 < 300 Millisekunden.
	•	Protokolle: Durchsuche Protokolle nach request_id und duration_ms, prüfe auf Ausreißer.
	•	Kanten-Richtlinie: Lade die Platzhalterseite im Browser und prüfe in der Konsole, ob die Inhaltsrichtlinie greift (fehlende Quellen sollen geblockt werden).

⸻

Risiken in Phase 0 → Gegenmittel
	•	Werkzeugbrüche (Bun vs. einzelne Werkzeuge): Reserve-Spur über „Node“ ab Version 20 nur für Werkzeuge, nicht für die Programmierschnittstelle.
	•	Scheinerfolg ohne Metriken: Lieferkette bricht ab, wenn Metriken fehlen oder leer sind.
	•	Zerfransung der Verträge: Früh eigene Versionierung in packages/contracts, Tor in der Lieferkette prüft, ob eine Änderung brechend ist.

⸻

Nächste Phase (Vorschau auf Phase 1)
	•	Postgres + PostGIS installieren, Migrationen für zentrale Tabellen, NATS JetStream mit dauerhaften Verbrauchern und „Dead Letter Queue“; Outbox-Schema anlegen; erste Projektion.

⸻

∴essenz.kernΣ

Phase 0 liefert kein Produkt, sondern Verlässlichkeit: ein Projekt, das früh scheitert, wenn etwas schief ist, und dadurch später nicht kollabiert.

Ungewissheitsgrad

Niedrig. Kleinere Unbekannte: genaue Nebenstände der Werkzeuge (Kleinstversionen). Auswirkungen minimal.

Abschließende Leitfragen

– War dies die kritischstmögliche Erörterung?
Kontrastvektor: Alternative wäre ein vollständiger Schnellstart ohne Leitplanken—spart einen Tag, kostet Wochen beim ersten Brand.
Negationsprojektion: „Kein Budget, keine Tore, wir messen später“ – das wäre genau der Fehler, den Phase 0 verhindern soll.

– Wurde das Thema von allen Seiten beleuchtet?
Mögliche Lücke: formale Datenschutz-Folgenabschätzung fehlt (bewusst Phase 3+), ebenso echte Datenbank-Metriken (kommen in Phase 1).
```

### 📄 docs/fahrplan.md

**Größe:** 4.46 KB

```markdown
# Fahrplan – Grob & angepasst (Stand 06.09.2025)

---

## Phase 0 – Bootstrap & Guardrails (~1 Woche)
**Ziel:** Arbeitsfähiges Monorepo mit Stack-Grundgerüst und harten Schutzgeländern.  
**To-dos:**
- Repo-Struktur (`apps/`, `packages/`, `infra/`, `docs/`)
- API-Stub (Bun + Elysia): `/health`, `/metrics`, JSON-Logs, Request-IDs
- CI/CD hart: Lint, Typecheck, Unit-Test-Stub, Coverage ≥ 80 %, k6-Smoke, axe-CI, Security-Scans, Hono-Exit (nur Typecheck)
- Performance-Budgets fixieren: JS ≤ 70 KB, LCP ≤ 1.8 s, API p95 ≤ 150 ms
- Version-Review (Pins monatlich)  
**Milestone:** Main grün, Checks sichtbar in CI, Grafana-Stub.  
**DoD:** `/health` & `/metrics` ok, alle CI-Jobs grün, Coverage-Gate aktiv.

---

## Phase 1 – Fundament (DB, Events, Contracts) (~2 Wochen)
**Ziel:** Datenbasis + Eventing + Verträge.  
**To-dos:**
- DB: PostgreSQL 17.6 + PostGIS 3.6, H3-Spalten
- Migrationen: users, accounts, nodes, threads, yarn, outbox
- NATS 2.11 JetStream: durable, queue groups, DLQ
- Contracts (Zod) + erste Core-Handler (fetch-first)
- Tests: Migrationen, Outbox→NATS→Consumer  
**Milestone:** Event-PoC (Send→Consume) sichtbar in Logs/Metrics (<1 % Verlust)  
**DoD:** Migrationen laufen, DLQ konfiguriert, verifizierter Test-Account möglich.

---

## Phase 2 – Domäne (Garnrolle, Knoten, Fäden, Garn, Goldfäden) (~3 Wochen)
**Ziel:** Erste Kernobjekte der Domäne funktionsfähig.  
**To-dos:**
- Garnrolle: Marker auf Karte, 7-Sek.-Rotation bei Aktionen
- Knoten CRUD: Typen, Geometrie, Tags
- Fäden: Gespräch, Antrag, Abstimmung, Gold, Delegation
- Fade→Garn: Verzwirnen → permanente Inhalte
- Goldfäden: Verbindung Garnrolle ↔ Gewebekonto
- Projektionen/Queries: H3-Aggregationen, Counters pro Typ  
**Milestone:** Erster Goldfaden sichtbar, Event→Projection→Query-Latenz < 5 s  
**DoD:** End-to-End-Demo; Kernrouten p95 < 150 ms.

---

## Phase 3 – Governance (7+7, Delegation, Moderation, Strukturknoten, RoN) (~3 Wochen)
**Ziel:** Entscheidungs- und Moderationslogik umsetzen.  
**To-dos:**
- 7+7-Verfahren, Quoren, Fristen, Abstimmungen
- Delegation: Liquid (Ablauf 4 Wochen, Widerruf)
- Moderation: Meldung, Freeze, Webrat-Abstimmung
- Strukturknoten: Gewebekonto, Webrat, Nähstübchen
- RoN: Opt-in, 84-Tage-Anonymisierung (Dry-Run)
- Privacy-Hooks: Sichtbarkeits-Flags, Opt-Outs  
**Milestone:** Simulierter Antrag durchläuft 7+7-Verfahren  
**DoD:** Auditierbare Event-Kette, RBAC-Claims greifen, RoN-Dry-Run läuft.

---

## Phase 4 – Frontend-UX (Map, Suche/Filter, Zeitleiste, PWA) (~3–4 Wochen)
**Ziel:** Mobile-First UI mit Karte, Suche, Filter, Timeline.  
**To-dos:**
- MapLibre lazy; serverseitiges H3-Clustering; Schwelle für MVT-Tiles definieren
- Suche & Filter: Text, Typ-Chips, Tags, Entfernung
- Zeitleiste: Zeit-Slider, Backfill (historische Projektionen)
- PWA: Service Worker, Asset-Cache, lokale Fonts, Icon-Sprites
- A11y: WCAG AA, axe-CI
- Privacy-UI: Opt-Outs, Sichtbarkeits-Switches  
**Milestone:** E2E-Flow Suche→Highlight→Knoten-Detail→Zeitleiste  
**DoD:** Lighthouse-CI grün (Mobile), LCP ≤ 1.8 s, axe-CI fehlerfrei.

---

## Phase 5 – Observability & Prod-Setup (~2 Wochen)
**Ziel:** Betriebsreifes System.  
**To-dos:**
- Prometheus, Loki (30d Retention), Grafana Dashboards
- OTEL-Tracing (5–10 % Sampling)
- Caddy (HTTP/3, TLS, CSP/HSTS, mTLS intern)
- Docker Compose: API, Worker, Web, Postgres, NATS, Observability
- Backups: pg_dump + WAL; Restore-Probe
- Firewall (80/443), Secrets, SLA-Alerts  
**Milestone:** Prod-Compose „up“ mit Dashboards, SLA-Checks  
**DoD:** Restore-Probe bestanden, SLA-Alerts aktiv, Zero-Downtime-Deploy.

---

## Phase 6 – Wachstum & Erweiterungen (~4+ Wochen)
**Ziel:** Skalierung & Föderation.  
**To-dos:**
- Ortswebereien: eigene Gewebekonten, Unterseiten
- Partizipartei (Experiment-Modul)
- Skalierung: DB-Replicas, NATS Autoscaling, MVT-Tiles
- Privacy-Feinschliff: Log-Retention, RoN-Workflows
- Public SDKs/Clients, OpenAPI-Export  
**Milestone:** Lasttest 10× Basis-Traffic (p95 < 150 ms)  
**DoD:** Zielwerte unter Last erfüllt, SDK/API veröffentlicht.

---

## Querschnitt – Wächter (dauerhaft)
- CI Gates: Lint/Typecheck, Coverage ≥ 80 %, k6-Smoke, axe-CI, semgrep, npm audit
- Budgets: JS ≤ 70 KB; LCP ≤ 1.8 s; API p95 ≤ 150 ms; DB p95 ≤ 50 ms; Outbox-Lag < 1 s
- Exit-Haken: Hono monatlich compile-testen
- Security/Privacy: Logs ≤ 30 d; CSP/HSTS; Secrets-Rotation
- Reviews: monatlich Privacy/A11y-Review & Version-Pins
- E2E-Tests: ab Phase 4 Pflicht, CI bricht bei Rot ab
```

### 📄 docs/inhalt.md

**Größe:** 9.47 KB

```markdown
# Inhalt (MANDATORISCH)

## Was bedeutet Weltweberei?

welt = althochdeutsch weralt = menschenzeitalter
weben = germanisch webaną, indogermanisch webʰ- = flechten, verknüpfen, bewegen

Guten Tag,

schön, dass du hergefunden hast! Tritt gerne ein in unser Weltgewebe oder schau dir erstmal an, um was es hier überhaupt geht.

Anschauen kostet nichts, beitreten (bald erst möglich) auch nicht, dabei sein auch nicht, nichts kostet irgendetwas. Du kannst nach eigenem Ermessen und kollektiven Gutdünken von diesem Netzwerk an gemeinsamen Ressourcen profitieren, bist gleichzeitig aber natürlich ebenso frei der Gemeinschaft etwas von dir zurückzugeben – was auch immer, wie auch immer.

Weltweberei ist der Name dieses Konzeptes eines sichtbaren, gemeinschaftlich ausgehandelten Zusammenwirkens von Nachbarschaften, versammelt um ein gemeinsames Konto. weltgewebe.net ist die Leinwand (Karte), auf der die jeweiligen Aktionen, Wünsche, Kommentare und Verantwortungsübernahmen der Weltweber visualisiert werden – als dynamisch sich veränderndes Geflecht von Fäden und Knoten.

## Wie funktioniert das Weltgewebe?

Jeder kann auf dem Weltgewebe (Online-Karte) alles einsehen. Wer sich mit Namen und Adresse registriert, der bekommt eine Garnrolle auf seinen Wohnsitz gesteckt. Diese Rolle ermöglicht es einem Nutzer, sich aktiv ins Weltgewebe einzuweben, solange er eingeloggt (sichtbar durch Drehung der Rolle) ist. Er kann nun also neue Knoten (auf der Karte lokalisierte Informationsbündel, beispielsweise über geplante oder ständige Ereignisse, Fragen, Ideen) knüpfen, sich mit bestehenden verbinden (Zustimmung, Interesse, Ablehnung, Zusage, Verantwortungsübernahme, etc.), an Gesprächen (Threads auf einem Knoten) teilnehmen, oder Geld an ein Ortsgewebekonto (Gemeinschaftskonto) spenden.

Jede dieser Aktionen erzeugt einen Faden, der von der Rolle zu dem jeweiligen Knoten führt. Jeder Faden verblasst sukzessive binnen 7 Tagen. Auch Knoten lösen sich sukzessive binnen 7 Tagen auf, wenn es ein datiertes Ereignis war und dieses vorbei ist, oder wenn seit 7 Tagen kein Faden (oder Garn) mehr zu diesem Knoten geführt hat. Führt jedoch ein Garn zu einem Knoten (siehe unten), dann besteht dieser auch permanent, bis das letzte zu ihm führende Garn entzwirnt ist. Kurzum: Knoten bestehen solange, wie noch etwas Garn oder Faden zu ihm führt.

### Benutzeroberfläche und Navigation

Der linke Drawer enthält den Webrat und das Nähstübchen. Hier wird über alle ortsunabhängigen Themen beraten (und abgestimmt. Generell kann jeder jederzeit Abstimmungen einleiten). Im Nähstübchen wird einfach (orts-/kartenunabhängig) geplaudert. Das Ortsgewebekonto (oberer Slider) ist das Gemeinschaftskonto. Hier gehen sowohl anonyme Spenden, als auch sichtbare Spenden (als Goldfäden von der jeweiligen Rolle) ein. Hier, wie auch überall im Gewebe können Weber Anträge (auf Auszahlung, Anschaffung, Veränderung, etc.) stellen.

Solch ein Antrag ist ebenso durch einen speziellen Antragsfaden mit der Rolle des Webers verbunden und enthält sichtbar einen 7-Tage Timer. Nun haben alle Weber 7 Tage lang Zeit Einspruch einzulegen. Geschieht dies nicht, dann geht der Antrag durch, bei Einspruch verlängert sich die Entscheidungszeit um weitere 7 Tage bis schlussendlich abgestimmt wird. Jeder Antrag eröffnet automatisch einen Raum mitsamt Thread und Informationen. Überhaupt entsteht mit jedem Knoten ein eigener Raum (Fenster), in dem man Informationen, Threads, etc. nebeneinander gestalten kann. Alles, was man gestaltet, kann von allen anderen verändert werden, es sei denn man verzwirnt es. Dies führt automatisch dazu, dass der Faden, der zu dem Knoten führt und von der Rolle des Verzwirners ausgeht, zu einem Garn wird. Solange also eine Verzwirnung besteht, solange kann ein Knoten sich nicht auflösen. Die Verzwirnung kann einzelne Elemente in einem Knoten oder auch den gesamten Knoten betreffen.

Unten ist eine Zeitleiste. Man kann hier in Tagesschritten zurückspringen und vergangene Webungen sehen. Auf der rechten Seite ist ein Slider mit den Filterkästchen für die toggelbaren Ebenen. Ecke oben rechts: eigene Kontoeinstellung (nicht zu verwechseln mit Ortsgewebekontodarstellung oben). Man hat in seiner eigenen Garnrolle einen privaten Bereich (Kontoeinstellungen, etc.) und einen öffentlich einsehbaren. In dem öffentlich einsehbaren kann man unter anderem Güter und Kompetenzen, die man der Gesamtheit zur Verfügung stellen möchte, angeben.

Über eine Suche im rechten Drawer kann man alle möglichen Aspekte suchen. Sie werden per Glow auf dem verorteten Knoten oder Garnrolle und auf einer Liste dargestellt. Die Liste ist geordnet nach Entfernung zur Bildmitte bei Suchbeginn. Von der Liste springt man zu dem verorteten Knoten oder Garnrolle, wenn man den Treffer anklickt.

All diese Ebenen (links, oben, Ecke rechts oben, rechts) werden aus der jeweiligen Ecke oder Kante herausgezogen. Die Standardansicht zeigt nur die Karte. Kleine Symbole zeigen die herausziehbaren Ebenen an.

### Fadenarten und Knotentypen

Es gibt unterschiedliche Fadenarten (in unterschiedlichen Farben):

- **Gesprächsfaden** - für Kommunikation und Diskussion
- **Gestaltungsfaden** - neue Knoten knüpfen, Räume gestalten (mit Informationen versehen, einrichten, etc.)
- **Veränderungsfaden** - wenn man bestehende Informationen verändert
- **Antragsfaden** - für offizielle Anträge im System
- **Abstimmungsfaden** - für Teilnahme an Abstimmungen
- **Goldfaden** - für Spenden und finanzielle Beiträge
- **Meldefaden** - für Meldungen problematischer Inhalte

Alle sind verzwirnbar, um aus den Fäden ein permanentes Garn zu zaubern.

Auch gibt es unterschiedliche Knotenarten:

- **Ideen** - Vorschläge und Konzepte
- **Veranstaltungen** (diversifizierbar) - Events und Termine
- **Einrichtungen** (diversifizierbar) - physische Orte und Gebäude
- **Werkzeuge** - Hilfsmittel und Geräte
- **Schlaf-/Stellplätze** - Übernachtungs- und Parkmöglichkeiten
- etc.

Diese Knotenarten sind auf der Karte filterbar (toggelbar).

## Organisation und Struktur

Weltweberei ist das Konzept. Realisiert wird es durch Ortswebereien, welche sich um ein gemeinsames Gewebekonto versammeln. Jede Ortsweberei hat eine eigene Unterseite auf weltgewebe.net.

### Accounts und Nutzerkonten

Die Verifizierung übernimmt ein Verantwortlicher der Ortsweberei (per Identitätsprüfung etc.). Damit wird dem Weber ein Account erstellt, den er beliebig gestalten kann. Es gibt einen öffentlich einsehbaren und einen privaten Bereich. Der Account wird als Garnrolle auf seiner Wohnstätte visualisiert.

**Wichtige Unterscheidung:**

- Rolle ≠ Funktion im Gewebe
- Rolle = Kurzform für Garnrolle = auf Wohnsitz verorteter Account

Das System der Weltweberei kommt ohne Währungsalternativen oder Creditsysteme aus. Sichtbares Engagement + eingebrachte bzw. einzubringende Ressourcen (also geleistete und potenzielle Webungen) sind die Währung!

### Ortsgewebekonto

Dies ist das Gemeinschaftskonto der jeweiligen Ortswebereien.

Per Visualisierung im Weltgewebe jederzeit einsehbar.

Hier gehen Spenden ein und werden Anträge auf Auszahlung gestellt, die – wie alles im Weltgewebe – dem Gemeinschaftswillen zur Disposition stehen.

### Partizipartei

Der politische Arm der jeweiligen Ortswebereien. Der Clou: Alles politische geschieht unter Live-Beobachtung und -Mitwirkung der Weber und anderer Interessierter (diese jedoch ohne Mitwirkungsmöglichkeit).

Die Arbeit der Fadenträger (Mandatsträger) und dessen Fadenreicher (Sekretäre, die den Input aus dem Gewebe aufbereiten und an den Fadenträger weiterreichen) wird während der gesamten Arbeitszeit gestreamt. Weber können live im Stream-Gruppenchat ihre Ideen (gefiltert durch Aufwertung/Abwertung der Mitweber und möglicherweise unterstützt / geordnet durch eine Plattform-Künstliche Intelligenz) und Unterstützungen einbringen. Jede Funktion, jeder Posten kann – wie alles in dem Weltgewebe – per Antrag umbesetzt oder verändert werden. Jeder Weber (auch die kleinen) haben eine Stimme. Diese können sie temporär an andere Weber übertragen. Das bedeutet, dass diejenigen, an die die Stimmen übertragen wurden, bei Abstimmungen dementsprechend mehr Stimmmacht haben.

Auch übertragene Stimmen können weiterübertragen werden. Übertragungen enden 4 Wochen nach Inaktivität des Stimmenverleihenden oder durch dessen Entscheidung.

## Kontakt / Impressum / Datenschutz

**E-Mail-Adresse:** kontakt@weltweberei.org
Schreib gerne, wenn du interessiert bist, Fragen, Anregungen oder Kritik hast. Oder willst du gar selber eine Ortsweberei gründen oder dich anderweitig beteiligen?

**Telefon:** +4915563658682
Aktuell benutze ich WhatsApp und Signal

**Verantwortlicher:** Alexander Mohr, Huskoppelallee 13, 23795 Klein Rönnau

**Datenschutz:** Das Weltgewebe ist so konzipiert, dass keine Daten erhoben werden, ohne dass du sie selbst einträgst. Es gibt kein Tracking, keine versteckten Cookies, keine automatische Profilbildung. Sichtbar wird nur das, was du freiwillig sichtbar machst: Name, Wohnort, Verbindungen im Gewebe. Deine persönlichen Daten kannst du jederzeit verändern oder zurückziehen. Die Verarbeitung deiner Daten erfolgt auf Grundlage von Artikel 6 Absatz 1 lit. a und f der Datenschutzgrundverordnung – also: Einverständnis & legitimes Interesse an sicherer Gemeinschaftsorganisation.

## Technische Umsetzung

Ich arbeite an einem iPad und an einem Desktop PC.

Die technische Umsetzung soll maximale Kontrolle, Skalierbarkeit und Freiheit berücksichtigen. Es soll stets die perspektivisch maximalst sinnvolle Lösung umgesetzt werden.
```

### 📄 docs/webstack.md

**Größe:** 5.40 KB

```markdown


# webstack.md — Ideale Synthese für Weltgewebe (Stand 06.09.2025)

∴subtext: Der Motor ist Bun/Elysia, die Straße ist Hetzner, die Karte zeichnet PostGIS, und Caddy hält die Leitplanken.

---

## 1) Leitplanken
- **Performance (mobil-first):**  
  LCP ≤ 1,8 s (4G), Start-JS ≤ 70 KB, API p95 ≤ 150 ms, DB p95 ≤ 50 ms, Outbox-Lag < 1 s
- **Kosten:** Hetzner-First, Open Source, ~50 €/Monat Basis
- **Architektur:** Event-Driven (Outbox → NATS JetStream + DLQ), ACID-Core (PostgreSQL + PostGIS), Vertrags-First (Zod)
- **Sicherheit/Privacy:** OIDC (Zitadel), kurzlebige JWT, RBAC, CSP/HSTS, mTLS intern, Logs ≤ 30 Tage
- **Barrierefreiheit:** WCAG AA enforced in CI (axe-CI)

---

## 2) Versionen (gepinnt)
- **Frontend:** SvelteKit ^2.37.x, Svelte ^5.28.x, Tailwind, MapLibre GL JS ^5.7.x
- **API/Edge:** Bun 1.2.x + Elysia ^1.3.x (Core), Hono ^4.9.x als Exit-Haken (nur Typecheck/Edge-Canary)
- **Events:** NATS 2.11.x (JetStream, Queue Groups, DLQ)
- **DB/Geo:** PostgreSQL 17.6, PostGIS 3.6, H3-Spalten, Drizzle ^0.44.x
- **Auth:** Zitadel ^4.1 (OIDC PKCE, JWT, RBAC, 2FA)
- **Observability:** Prometheus, Loki (30 Tage), Grafana, OTEL (5–10 % Sampling)
- **Infra:** Caddy 2.10 (HTTP/3, TLS, zstd, CSP/HSTS, mTLS intern), Docker Compose

**Optionale Booster:**  
- **Tiles:** Martin (Rust) oder Tegola (Go) → MVT direkt aus PostGIS  
- **Geo/ML-Micro:** Django + GeoDjango/DRF, isoliert, nur bei Bedarf

---

## 3) Architektur

- **Frontend (SvelteKit):** PWA (Service Worker, Asset-Cache), lokale Fonts, Icon-Sprites; MapLibre lazy; H3-Clustering; ab hoher Dichte → MVT
- **API (Elysia auf Bun):** Fetch-first, Zod-Contracts, JWT/RBAC, Outbox-Publisher → NATS, JSON-Logs + OTEL
- **Worker (Elysia):** Durable consumer, Queue Groups, Idempotenz, DLQ + Backoff
- **DB (Postgres + PostGIS):** ACID, GiST/GIN-Indizes, MatViews für Hot-Queries, Read-Replicas, ST_DWithin
- **Auth (Zitadel):** Access ≤10 min, Refresh ≤24h, Claims-basiertes RBAC
- **Observability:** /metrics, Loki (30 Tage), Grafana-Dashboards, OTEL
- **Infra:** Caddy terminates TLS/HTTP3 + CSP/HSTS + mTLS; Compose orchestriert

```ascii
SvelteKit (PWA, MapLibre lazy)
        │  HTTP/3 (TLS, zstd)
        ▼
    Caddy ──────────────────────────────────────────────┐
        │                                               │
        │ /api/*                                        │ /tiles/*
        ▼                                               ▼
 Elysia API (Bun)  ─ Outbox → NATS JetStream ← Worker    Martin/Tegola (optional)
   JWT/RBAC, Zod          durable, QG, DLQ
        │
        ▼
 PostgreSQL 17.6 + PostGIS 3.6 (H3, MatViews, Replicas)
        │
        └──► Prometheus / Loki / Grafana (OTEL)


⸻

4) Repo-Layout

.
├─ apps/
│  ├─ web/           # SvelteKit + MapLibre
│  ├─ api-elysia/    # API & Worker (Bun + Elysia)
│  └─ api-hono/      # Exit-Haken (Typecheck only)
├─ packages/
│  ├─ contracts/     # Zod Schemas (Client/Server shared)
│  └─ core/          # Frameworkfreie Handler/Utils
├─ infra/            # compose, caddy, prom, loki, grafana, nats, postgres, (martin)
├─ .github/workflows/ci.yml
├─ scripts/          # k6, lighthouse-ci, semgrep, release-diff
└─ docs/             # webstack.md, fahrplan.md


⸻

5) CI/CD (hart, prüfbar)

Pipeline (GitHub Actions):
Lint → Typecheck → Unit (bun test —coverage) → Coverage-Gate ≥80 % → Build → API Spin-up → k6 Smoke (/health,/metrics) → axe-CI → semgrep → npm audit → Hono-Typecheck.

Extras: monatlicher Pins-Review (automatischer Issue + Release-Diff), Budgets als Code (Lighthouse-CI mobil, k6 thresholds, Grafana-Alerts).

⸻

6) Security & Privacy
	•	CSP (inkl. MapLibre-Sources), HSTS, Referrer-Policy, COOP/COEP
	•	mTLS intern, Firewall: nur 80/443
	•	Secrets via Env-Manager
	•	Backups: pg_dump + WAL + Restore-Tests
	•	Logs: strukturierte JSON-Logs, PII-Redaction am Ingress, Retention ≤30 Tage

⸻

7) Performance-Playbook
	•	Frontend: Route-Level Code-Split, Critical-CSS, lokale Fonts, Icon-Sprites, MapLibre Style-Pruning
	•	API: fetch-first, minimal Middleware, JSON-Streaming
	•	DB: ST_DWithin + GiST, BBOX-Vorfiltro, Hot-Queries als MatViews (FAST-Refresh)
	•	Events: Dedup-Keys, Exponential Backoff, DLQ-Dashboards + Replay
	•	Tiles: Ab hoher Punktdichte → MVT; kleine Daten → FlatGeobuf

Budgets: Start-JS ≤70 KB; API p95 ≤150 ms; DB p95 ≤50 ms; Outbox-Lag <1 s; CLS <0,1

⸻

8) Upgrade-/Fallback-Pfade
	•	Geo-Power: Martin/Tegola aktivieren → MapLibre auf MVT
	•	Geo/ML: optional Django-Micro
	•	Edge: Hono-Adapter für spätere Edge-Routen (Workers etc.)
	•	Vendor-Switch: Caddy/Compose/Volumes portabel; Postgres ggf. managed

⸻

9) Governance-MVP

Kernobjekte & Verfahren (Garnrollen, Fäden, Garn, Delegationen, 7+7-Modell, RoN) sind Roadmap-Bestandteil, bleiben stack-entkoppelt und werden über Contracts versioniert ￼ ￼.

⸻

10) Fazit

Optimaler Kern: Bun/Elysia + SvelteKit + MapLibre + NATS + Postgres/PostGIS + Drizzle + Zitadel + Prom/Loki/Grafana + Caddy.
Optionale Booster: Martin/Tegola (MVT), Django-Micro (Geo/ML).

Warum: schnell, günstig, mobil-stark, mit klaren Evolutionspfaden – ohne Lock-in.

⸻

∴essenz.kernΣ
Leicht, schnell, modular – Verträge im Zentrum, Events im Fluss, Geo direkt aus PostGIS. Alles Weitere nur optional.

---

```

### 📄 docs/wg.md

**Größe:** 592.00 B

```markdown

## Reload mit `--here`
`wg reload [1|2] --here` startet die neue Shell **genau im aktuellen Unterordner** (falls dieser innerhalb des Repos liegt), sonst im Repo-Root.
- `1` = Bash neu
- `2` = Termux: Debian-Proot (Bun) im Ziel

## Heal
`wg heal` hilft bei divergenten Branches:
- **1** lokal bevorzugen → `git pull -s recursive -X ours`
- **2** remote bevorzugen → `git pull -s recursive -X theirs`
- **3** Rebase → `git pull --rebase`
- **4** Fast-Forward only → `git pull --ff-only`

Vorher wird ein Stash angelegt (falls der Arbeitsbaum dirty ist) und danach wieder eingespielt.
```

### 📄 docs/zusammenstellung.md

**Größe:** 9.83 KB

```markdown
# Zusammenstellung (MANDATORISCH)

Das Weltgewebe: Eine Systematische Zusammenfassung

Das Weltgewebe ist eine kartenbasierte soziale Infrastruktur, die als eine Art Demokratie-Engine auf einer interaktiven Karte konzipiert ist. Jeder Beitrag eines Nutzers wird als "Faden" visualisiert. Die Plattform basiert auf den Kernprinzipien der radikalen Transparenz, Freiwilligkeit, technischer Absicherung durch Event-Sourcing und einem integrierten Datenschutzkonzept.

I. Grundprinzipien und Philosophie

- Alles ist ein Event: Jede Aktion im System wird als ein unveränderliches, signiertes Ereignis in einer Hash-Kette gespeichert (Event-Sourcing).
- Radikale Transparenz: Grundsätzlich sind alle Aktionen öffentlich sichtbar. Ausgenommen sind private Informationen im Nutzerkonto und private Nachrichten zwischen Nutzern.
- Freiwilligkeit: Die Teilnahme am Weltgewebe erfolgt ausschließlich nach informierter Zustimmung.
- Datenschutz (Privacy by Design): Es findet keine verdeckte Datensammlung statt, also keine Cookies, kein Tracking und keine automatische Profilbildung. Sichtbar ist nur, was Nutzer bewusst eintragen, wie Name, Wohnort und Verbindungen. Die rechtliche Grundlage für die Datenverarbeitung bilden die Datenschutzgrundverordnung-Artikel 6 Abs. 1 lit. a und f.
- Währungskonzept: Es gibt keine künstlichen Credits oder Alternativwährungen. Die eigentliche "Währung" ist sichtbares Engagement in Form von Fäden und Garn sowie die von Nutzern eingebrachten Ressourcen. Spenden können zusätzlich über "Goldfäden" sichtbar gemacht werden.

II. Das Domänenmodell: Nutzer, Inhalte und Struktur
Nutzer (Garnrollen)

- Nutzeraccounts (Rollen): Nutzer werden als "Garnrollen"-Icon an ihrem Wohnort auf der Karte visualisiert. Jede Aktion führt dazu, dass sich diese Rolle für alle sichtbar dreht.
- Verifizierung: Accounts werden von Verantwortlichen einer lokalen "Ortsweberei" durch eine Identitätsprüfung verifiziert und erstellt.
- Profilbereiche: Jeder Account verfügt über einen privaten Bereich für Kontoinformationen und einen öffentlichen Raum. Im öffentlichen Bereich können Nutzer Informationen über sich selbst sowie Güter und Kompetenzen eintragen, die sie der Gemeinschaft zur Verfügung stellen möchten.
  Inhalte (Knoten, Fäden, Garn)
- Knoten: Dies sind ortsbezogene Bündel von Informationen, wie Ideen, Veranstaltungen, Ressourcen, Werkzeuge oder Schlafplätze. Jeder Knoten eröffnet einen eigenen Raum, der Threads, Informationen und Anträge enthalten kann. Informationen können alternativ auch direkt auf der eigenen Garnrolle verortet werden. Knoten sind auf der Karte filter- und einblendbar.
- Fäden: Jede Nutzeraktion erzeugt einen "Faden" von der Garnrolle des Nutzers zu einem Knoten. Es gibt verschiedene Faden-Typen, darunter Gesprächs-, Gestaltungs-, Änderungs-, Antrags-, Abstimmungs-, Gold-, Melde- und Delegationsfäden. Delegationsfäden verlaufen von einer Garnrolle zu einer anderen. Nebeneinanderliegende Fäden und Garne, die von einer Rolle zu einem Knoten führen, überlappen sich zunehmend, um zu dicke Linien zu vermeiden.
- Vergänglichkeit und Beständigkeit (Garn): Fäden verblassen sukzessive innerhalb von 7 Tagen, wenn sie nicht durch einen Klick auf den "Verzwirnungsbutton" zu "Garn" gemacht werden. Verzwirnte Fäden (Garn) sind dauerhaft und schützen Inhalte sowie den gesamten Knoten vor Veränderung und Auflösung.
  Strukturknoten
  Dies sind permanente und immer sichtbare Knoten für zentrale Funktionen:
- Gewebekonto: Dient der Finanzverwaltung und der Übersicht über Goldfäden.
- Webrat: Der Ort für Governance, Anträge und die Übersicht über Delegationen. Alle Abstimmungen sind hier ebenso einsehbar und man kann daran teilnehmen.
- Nähstübchen: Ein ortsunabhängiger Raum für die allgemeine Kommunikation.
- RoN-Platzhalter: Ein spezieller Knoten, an dem anonymisierte Inhalte nach 84 Tagen gesammelt werden.

III. Zeitlichkeit, Sichtbarkeit und Anonymisierung

- 7-Sekunden-Rotation: Nach jeder Aktion dreht sich die Garnrolle des Nutzers für 7 Sekunden sichtbar auf der Karte.
- 7-Tage-Verblassen: Fäden, die nicht zu Garn verzwirnt werden, verblassen innerhalb von 7 Tagen sukzessive. Knoten, zu denen 7 Tage lang kein neuer Faden führt, lösen sich ebenfalls in diesem Zeitraum sukzessive auf.
- Anonymisierung (RoN-System):
  - Nutzer können per Opt-in festlegen, dass ihre Beiträge nach x Tagen automatisch anonymisiert werden. Der Autorenname wird dann durch "RoN" (Rolle ohne Namen) ersetzt.
  - Die anonymisierten Fäden führen dann nicht mehr zur ursprünglichen Garnrolle, sondern zum zentralen RoN-Platzhalter. Das Wissen bleibt so im Gewebe erhalten.
- Ausstiegsprozess: Wenn ein Nutzer die Plattform verlässt, durchlaufen alle seine Daten den RoN-Prozess. Beiträge, die jünger als x Tage sind, bleiben so lange namentlich sichtbar, bis diese Frist erreicht ist. Am Ende wird die Garnrolle des Nutzers gelöscht.
- Eigene Beiträge und Aktionen können selbstverständlich jederzeit gelöscht werden

IV. Governance und Demokratische Prozesse

- 7+7-Modell für Anträge:
  - Ein gestellter Antrag wird mit einem 7-Tage-Timer sichtbar.
  - Erfolgt innerhalb dieser Frist kein Einspruch, wird der Antrag automatisch angenommen.
  - Bei einem Einspruch beginnt eine weitere 7-tägige Abstimmungsphase, in der eine einfache Mehrheit entscheidet. Abstimmungen sind öffentlich und namentlich einsehbar, optional mit Begründung.
- Delegation (Liquid Democracy): Nutzer können ihre Stimme 1:1 an einen anderen Nutzer übertragen. Diese Delegationen werden als gestrichelte Pfeile zwischen den Garnrollen visualisiert und verfallen nach 4 Wochen Inaktivität des Delegierenden. Für eine spätere Phase (B) ist eine transitive Delegation mit Zykluserkennung (Cycle-Detection) geplant. Eine direkte Stimmabgabe überschreibt dabei temporär die Delegation. Rollen, die Delegationen empfangen haben, zeigen deren Gewicht an.
- Moderation ("Legal Freeze"): Strafbare Inhalte können über einen "Melden"-Button gemeldet werden, was ebenfalls einen Faden erzeugt. Bei Verdacht auf eine Straftat erfolgt ein sofortiger Freeze mit gerichtsfester Beweissicherung. Der gemeldete Inhalt wird für 24 Stunden eingeklappt und im Webrat sowie am Ort des Inhalts zur Abstimmung gestellt. Eine einfache Mehrheit entscheidet über die weitere Vorgehensweise. Eine Entfernung erfolgt nur, wo es rechtlich geboten ist, und nach Abschluss des Verfahrens wird ein öffentlicher Folge-Antrag gestellt.
- Politischer Arm (Partizipartei): Jede Ortsweberei kann einen politischen Arm gründen, die "Partizipartei". Mandatsträger ("Fadenträger") und ihre Helfer ("Fadenreicher") arbeiten unter permanenter Live-Übertragung. Die Bürgerbeteiligung wird durch einen Chat mit Aufwertung/Abwertung und optionaler Künstliche Intelligenz-Unterstützung ermöglicht. Jede Funktion und jeder Posten kann per Antrag verändert oder abgewählt werden.

V. Benutzeroberfläche und Nutzererlebnis

- Karten-Interface: Die primäre Oberfläche ist eine Vollbildkarte (MapLibre GL).
- Drawer-System:
  - Links: Zugriff auf Webrat und Nähstübchen (Governance und Kommunikation).
  - Rechts: Filter für Knoten- und Fadenarten, ein Zeitfenster und ein Suchmenü.
- Suchfunktion: Über das Suchmenü können die von Nutzern zur Verfügung gestellten Güter und Kompetenzen abgefragt werden. Treffer werden als aufleuchtende Rollen oder Knoten auf der Karte sowie in einer nach Entfernung sortierten Liste angezeigt. Ein Klick auf einen Listeneintrag zentriert die Karte auf den entsprechenden Nutzer.
- Widgets: Oben mittig befindet sich das Gewebekonto-Widget (Saldo, Bewegungen), oben rechts der Zugang zum eigenen Konto und zur Verifikation.
- Zeitleiste: Eine Zeitachse am unteren Bildschirmrand ermöglicht die Rückschau auf vergangene Aktivitäten ("Webungen").

VI. Organisation und Technische Architektur

- Lokale Organisation (Ortswebereien): Das Weltgewebe wird durch lokale "Ortswebereien" konkret umgesetzt. Jede dieser Gruppen verfügt über ein eigenes Gemeinschaftskonto (Gewebekonto) und eine Unterseite auf weltgewebe.net. Föderationen von Ortswebereien sind vorgesehen.
- Technischer Stack und Verortung: Die Architektur basiert auf Event-Sourcing mit NATS JetStream, PostgreSQL/PostGIS und Redis. Knoten und Rollen werden H3-basiert gespeichert, um räumliche Abfragen, Filter und Indizes zu ermöglichen.
- Hosting und Betrieb:
  - Der Betrieb ist für ein kleines Team (1–2 Personen) durch Automatisierung (Cronjobs, Healthchecks) ausgelegt.
  - Das Hosting erfolgt primär bei Hetzner, um Kosteneffizienz und Datenschutzgrundverordnung-Konformität zu gewährleisten ("Hetzner-First").
- Performance ("Mobile-First"): Die Plattform ist für Smartphones optimiert. Angestrebt werden ein Initial-Bundle von ≤ 90 KB und eine Time-to-Interactive von unter 2,5 Sekunden auf einer 3G-Verbindung. Weitere Performance-Ziele sind P95 API-Antwortzeiten von ≤ 300 ms und P95 Datenbankabfragen von ≤ 150 ms.
- Skalierung und Kosten: Ein Phasenmodell sichert die Skalierbarkeit von einem Single-Server (unter 200 €/Monat) bis hin zu Multi-Region-Clustern. Ziel ist es, die Kosten pro 1.000 Events unter 0,01 € zu halten.
- Hybrid-Indexierung: Live-Routen (z.B. /map, /feed) senden den X-Robots-Tag noindex, noarchive. Monatsarchive (z.B. /archive/YYYY-MM) sind hingegen als index, follow markiert und setzen ein rel="canonical"-Tag, um die Nachvollziehbarkeit zu gewährleisten.
- Monitoring, Alarme und Betriebspläne:
  - Metriken: Es werden Governance-Metriken (z.B. Teilnahmequote), RoN-Metriken (z.B. Transferrate) und Kosten-Metriken (z.B. €/aktiver Nutzer) überwacht. Es gibt Alarm-Regeln, z.B. bei Latenzen über 1000 ms oder wenn die Kosten in Phase A 200 € übersteigen.
  - Betriebspläne (Cronjobs): Governance-Timer laufen minütlich; Delegations-Prüfungen täglich um 01:00 Uhr; RoN-Prozesse um 02:00 Uhr und Kosten-Analysen um 03:00 Uhr. Für die Systemgesundheit gibt es die Endpunkte /health/live und /health/ready.
```

### 📄 e --continue

**Größe:** 1.51 KB

```
* [33m8ff8384[m[33m ([m[1;36mHEAD[m[33m -> [m[1;32mmain[m[33m)[m ci: Actions-Workflow (Bun/Elysia, Coverage-Gate, k6-Smoke)
[31m|[m * [33m385a6b0[m[33m ([m[1;31morigin/main[m[33m, [m[1;31morigin/HEAD[m[33m)[m wg: commit via Codespace 2025-09-06 18:47:09 UTC
[31m|[m[31m/[m  
* [33m8a5df35[m wg: mobile push 2025-09-06 18:32:22 UTC
* [33m4cf33e3[m wg: mobile push 2025-09-06 18:01:22 UTC
* [33m6d09685[m wg send: main 2025-09-06 19:09:36
* [33m94ca385[m wg send: main 2025-09-06 19:06:44
* [33me9b5ba5[m wg send: main 2025-09-06 19:06:03
* [33m8994457[m Update: zusammenstellung.md, inhalt.md, fahrplan.md, webstack.md
* [33m9cc6f10[m fix(devcontainer): Safe-Mode (keine externen Downloads, defensive postCreate/Start)
* [33mf43ab34[m fix(devcontainer): remove DinD feature for Codespaces; dedupe README hint
* [33m55f2050[m fix(devcontainer): remove root Dockerfile, enforce .devcontainer/Dockerfile
* [33ma73c681[m fix(devcontainer): remove root Dockerfile, enforce .devcontainer/Dockerfile
* [33m001c6f2[m let's roll
*   [33m7f516ea[m Merge branch 'main' of git@github.com:weltgewebe/weltgewebe-repo.git
[33m|[m[34m\[m  
[33m|[m * [33ma27d92e[m - old devcontainer
* [34m|[m [33m632c5d5[m Merge branch 'main' of git@github.com:weltgewebe/weltgewebe-repo.git
[35m|[m[34m\[m[34m|[m 
[35m|[m * [33m4a7c73a[m devcontainer
* [36m|[m [33m1ca1dfa[m nun aber
* [36m|[m [33m1216ce0[m neue webstack.md
* [36m|[m [33m321e664[m Create webstack.md
[36m|[m[36m/[m  
```

### 📄 package.json

**Größe:** 800.00 B

```json
{
  "name": "weltgewebe",
  "private": true,
  "packageManager": "bun@1.2.21",
  "workspaces": ["apps/*", "packages/*"],
  "engines": { "node": ">=20" },
  "scripts": {
    "dev:web": "bun --cwd apps/web run dev",
    "dev:api": "bun --cwd apps/api-elysia run dev",
    "dev": "concurrently -k -n web,api \"bun run dev:web\" \"bun run dev:api\"",
    "typecheck": "bun --cwd apps/api-elysia run check",
    "build": "bun --cwd apps/api-elysia run build",
    "infra:up": "docker compose -f infra/compose.yml up -d",
    "infra:down": "docker compose -f infra/compose.yml down -v",
    "infra:logs": "docker compose -f infra/compose.yml logs -f --tail=50"
  },
  "devDependencies": {
    "concurrently": "^9.0.1",
    "@biomejs/biome": "^1.9.4",
    "typescript": "^5.6.2",
    "zod": "^3.23.8"
  }
}
```

### 📄 packages/contracts/package.json

**Größe:** 172.00 B

```json
{
  "name": "@welt/contracts",
  "version": "1.0.0",
  "type": "module",
  "exports": "./src/index.ts",
  "devDependencies": { "zod": "^3.23.8", "typescript": "^5.6.2" }
}
```

### 📄 packages/contracts/src/index.ts

**Größe:** 410.00 B

```typescript
import { z } from "zod";

export const WGUserId = z.string().uuid();
export const WGRole = z.enum(["viewer","member","moderator","admin"]);

export const AuthClaims = z.object({
  sub: WGUserId,
  roles: z.array(WGRole).default(["viewer"]),
  iat: z.number(),
  exp: z.number()
});

export const EchoBody = z.object({
  message: z.string().min(1).max(200)
});
export type TEchoBody = z.infer<typeof EchoBody>;
```

### 📄 packages/core/package.json

**Größe:** 149.00 B

```json
{
  "name": "@welt/core",
  "version": "1.0.0",
  "type": "module",
  "exports": "./src/index.ts",
  "devDependencies": { "typescript": "^5.6.2" }
}
```

### 📄 packages/core/src/index.ts

**Größe:** 134.00 B

```typescript
export const ok = <T>(data: T) => ({ ok: true as const, data });
export const err = <E>(error: E) => ({ ok: false as const, error });
```

### 📄 README.md

**Größe:** 142.00 B

```markdown
# Weltgewebe – Monorepo (Bootstrap)
Leicht, schnell, modular: SvelteKit + Bun/Elysia + Postgres/PostGIS + NATS + Caddy + Prom/Loki/Grafana.
```

### 📄 scripts/codex-run.sh

**Größe:** 1.44 KB

```bash
#!/usr/bin/env bash
set -euo pipefail

export BUN_INSTALL="${BUN_INSTALL:-$HOME/.bun}"
export PATH="$BUN_INSTALL/bin:$PATH"
export NODE_ENV="${NODE_ENV:-production}"
export CI="${CI:-true}"

echo "▶ Install..."
bun install --frozen-lockfile
bun install -C apps/web || true
bun install -C apps/api-elysia || true

echo "▶ Typecheck..."
bunx tsc -p apps/web/tsconfig.json --noEmit
bunx tsc -p apps/api-elysia/tsconfig.json --noEmit

echo "▶ Lint..."
if [ -f biome.json ] || [ -f .biome.json ]; then
  bunx biome ci .
else
  echo "ℹ︎ Biome-Konfig nicht gefunden – Lint wird ausgelassen."
fi

echo "▶ Tests..."
bun test --coverage

echo "▶ Build smoke..."
bun run -C apps/web build
# falls api-elysia kein build-script hat, fällt es auf Typecheck zurück
if bun run -C apps/api-elysia build 2>/dev/null; then
  echo "API build ok"
else
  echo "API build-script fehlt – mache Typecheck als Smoke."
  bunx tsc -p apps/api-elysia/tsconfig.json --noEmit
fi

# Optional: Budgets & Smoke gegen laufende Services
if [ "${RUN_BUDGETS:-0}" = "1" ]; then
  echo "▶ Lighthouse Budgets (mobil)..."
  bash scripts/lighthouse/lhci-mobile.sh || (echo "⚠︎ LHCI Warnung" && exit 1)
fi

if [ "${RUN_K6:-0}" = "1" ]; then
  echo "▶ k6 Smoke..."
  if command -v k6 >/dev/null 2>&1; then
    k6 run scripts/k6/smoke.js
  else
    echo "⚠︎ k6 nicht installiert – Smoke wird übersprungen (RUN_K6=0 setzen, oder k6 installieren)."
  fi
fi

echo "✅ Codex-Run fertig."
```

### 📄 scripts/k6/smoke.js

**Größe:** 608.00 B

```javascript
import http from "k6/http";
import { check, sleep } from "k6";

export const options = {
  vus: 10,
  duration: "10s",
  thresholds: {
    http_req_failed: ["rate<0.01"],
    http_req_duration: ["p(95)<300"], // p95 < 300ms
  },
};

const BASE = __ENV.API_BASE || "http://localhost:8787";

export default function () {
  let res1 = http.get(`${BASE}/health`);
  check(res1, { "health 200": (r) => r.status === 200 });

  let res2 = http.get(`${BASE}/metrics`);
  check(res2, {
    "metrics 200": (r) => r.status === 200,
    "wg_up present": (r) => r.body && r.body.includes("wg_up"),
  });

  sleep(0.2);
}
```

### 📄 scripts/lighthouse/budgets.json

**Größe:** 218.00 B

```json
[
  {
    "path": "/*",
    "resourceSizes": [
      { "resourceType": "script", "budget": 70 }   // KB Start-JS
    ],
    "timings": [
      { "metric": "largest-contentful-paint", "budget": 1800 } // ms
    ]
  }
]
```

### 📄 scripts/lighthouse/lhci-mobile.sh

**Größe:** 607.00 B

```bash
#!/usr/bin/env bash
set -euo pipefail

export BUN_INSTALL="${BUN_INSTALL:-$HOME/.bun}"
export PATH="$BUN_INSTALL/bin:$PATH"

# Web-Preview starten (SvelteKit preview; Port 4173 Default)
# Wir bauen NICHT neu – Build ist bereits in codex-run erledigt.
bunx @lhci/cli autorun \
  --collect.startServerCommand="bun run -C apps/web preview --host --port 4173" \
  --collect.url="http://localhost:4173/" \
  --collect.numberOfRuns=1 \
  --collect.settings.preset=mobile \
  --collect.budgetsFile=scripts/lighthouse/budgets.json \
  --assert.preset="lighthouse:recommended" \
  --upload.target=temporary-public
```

### 📄 wg

**Größe:** 8.73 KB

```
#!/usr/bin/env bash
# wg – Weltgewebe CLI (PR-only, clean/heal/pr/reload/send)
# mobile-first • Termux-tolerant • Codespaces-ready

# ---------- Basics ----------
repo_default="$(cd "$(dirname "$0")" && pwd)"
repo_detected="$(git -C "$PWD" rev-parse --show-toplevel 2>/dev/null)"
repo="${repo_detected:-$repo_default}"

is_termux=0; case "$PREFIX" in */com.termux/*) is_termux=1 ;; esac
log(){ printf "%s\n" "$*"; }
die(){ printf "❌ %s\n" "$*" >&2; exit 1; }
ensure_repo(){ [ -d "$repo/.git" ] || die "Repo nicht gefunden: $repo"; }

# ---------- Helpers ----------
preflight(){
  cd "$repo" || exit 1
  # blockiere laufende Aktionen
  [ -d .git/rebase-apply ] || [ -d .git/rebase-merge ] && die "Rebase läuft (git rebase --continue | --abort)"
  [ -f .git/MERGE_HEAD ] && die "Merge läuft (Konflikte lösen + commit oder git merge --abort)"
  BRANCH="$(git rev-parse --abbrev-ref HEAD)"; [ "$BRANCH" = "HEAD" ] && die "Detached HEAD"
  UPSTREAM="$(git rev-parse --abbrev-ref --symbolic-full-name @{u} 2>/dev/null || true)"
  if [ -z "$UPSTREAM" ]; then
    if git show-ref --verify --quiet "refs/remotes/origin/$BRANCH"; then
      git branch --set-upstream-to "origin/$BRANCH" "$BRANCH" >/dev/null
      UPSTREAM="origin/$BRANCH"
    else
      UPSTREAM="origin/$BRANCH" # wird beim ersten Push angelegt
    fi
  fi
}

repo_path(){ # nur GitHub-URLs
  local u; u="$(git config --get remote.origin.url 2>/dev/null || true)"
  case "$u" in
    git@github.com:*) printf "%s" "${u#git@github.com:}" | sed "s/\.git$//" ;;
    https://github.com/*) printf "%s" "${u#https://github.com/}" | sed "s/\.git$//" ;;
    *) printf "" ;;
  esac
}

urlencode(){
  # bevorzugt python3, fallback: naive %20 etc.
  if command -v python3 >/dev/null 2>&1; then
    python3 - <<PY 2>/dev/null
import sys, urllib.parse
print(urllib.parse.quote(sys.stdin.read().strip()))
PY
  else
    sed -e "s/%/%25/g" -e "s/ /%20/g" -e "s/\"/%22/g" -e "s/#/%23/g" -e "s/&/%26/g"
  fi
}

auto_title(){
  local branch date last prefix changes
  branch="$(git rev-parse --abbrev-ref HEAD)"
  date="$(date +%Y-%m-%d)"
  last="$(git log -1 --pretty="%s" 2>/dev/null || echo "update")"
  prefix="feat"
  if [[ "$last" =~ ^(feat|fix|chore|docs|refactor|perf|test) ]]; then prefix="${BASH_REMATCH[1]}"; fi
  if git rev-parse --verify -q "refs/remotes/origin/$branch" >/dev/null 2>&1; then
    changes="$(git diff --name-only "origin/$branch"... 2>/dev/null | wc -l | awk "{print \$1}")"
  else
    changes="0"
  fi
  printf "%s: %s | %s | %s (%s changes)" "$prefix" "$branch" "$date" "$last" "$changes"
}

# ---------- Commands ----------
cmd_go(){
  ensure_repo
  log "🌍 Repo: $repo"
  (cd "$repo" && ls -1 . 1>/dev/null) && log "✅ erreichbar"
}

cmd_doctor(){
  ensure_repo
  log "🩺 $(git --version 2>/dev/null || echo git fehlt)"
  command -v curl >/dev/null 2>&1 && log "✅ curl: $(curl --version | head -n1)" || log "ℹ️ curl nicht gefunden (ok)"
  [ "$is_termux" -eq 1 ] && command -v proot-distro >/dev/null 2>&1 && log "✅ proot-distro" || true
}

# Vorschau: wg clean | Anwenden: wg clean --apply
cmd_clean(){
  ensure_repo; cd "$repo" || exit 1
  local apply=0; case "$1" in --apply|-y|--force) apply=1;; esac
  mapfile -t HITS < <(find . -path "./.git" -prune -o \( \
     -type f \( -name "*.bak" -o -name "*.tmp" -o -name "*.orig" -o -name "*.rej" -o -name "*~" -o -name ".DS_Store" -o -name "Thumbs.db" -o -name "*.swp" \) -o \
     -type d \( -name "node_modules" -o -name "dist" -o -name ".svelte-kit" -o -name "coverage" \) \
   \) -print 2>/dev/null)
  [ ${#HITS[@]} -eq 0 ] && { log "✅ Nichts zu säubern."; return 0; }
  if [ $apply -eq 0 ]; then
    log "👀 Vorschau (nichts gelöscht). Anwenden: wg clean --apply"
    printf "%s\n" "${HITS[@]}"
  else
    log "🗑️  Lösche ${#HITS[@]} Einträge…"
    printf "%s\n" "${HITS[@]}" | awk "{print length, \$0}" | sort -nr | cut -d" " -f2- | while read -r p; do
      [ -d "$p" ] && rm -rf -- "$p" || rm -f -- "$p"
    done
    log "✅ Clean fertig."
  fi
}

# Interaktiver Konflikt-Heiler
cmd_heal(){
  ensure_repo; preflight
  cd "$repo" || return 1
  echo "🧩 Strategie wählen:"
  echo "  1) lokal bevorzugen  (merge -X ours)"
  echo "  2) remote bevorzugen (merge -X theirs)"
  echo "  3) rebase"
  echo "  4) fast-forward only"
  read -p "➤ Auswahl [1/2/3/4]: " sel
  case "$sel" in
    1) git pull -s recursive -X ours ;;
    2) git pull -s recursive -X theirs ;;
    3) git pull --rebase ;;
    4) git pull --ff-only ;;
    *) log "Abbruch."; return 1 ;;
  esac || { log "⚠️ Konflikte/Fehler. Bitte manuell lösen."; return 1; }
  log "✅ Heal erfolgreich."
}

# PR-Link erzeugen (mit Auto-Titel)
cmd_pr(){
  ensure_repo; preflight
  cd "$repo" || exit 1
  local title="${*}"; [ -z "$title" ] && title="$(auto_title)"
  local path; path="$(repo_path)"
  [ -z "$path" ] && { log "ℹ️ PR-Link nicht ableitbar (Remote nicht GitHub)"; return 0; }
  local enc; enc="$(printf "%s" "$title" | urlencode)"
  local head; head="$(git rev-parse --abbrev-ref HEAD)"
  printf "🔗 PR-Link: https://github.com/%s/compare/main...%s?expand=1&title=%s\n" "$path" "$head" "$enc"
}

# PR-only Flow: nie direkt auf main
cmd_send(){
  ensure_repo; preflight
  cd "$repo" || exit 1

  # Nie direkt auf main arbeiten → Branch anlegen
  if [ "$BRANCH" = "main" ]; then
    local ts; ts="$(date -u +%Y%m%d-%H%M%S)"
    local newb="pr/feat-${ts}"
    log "⚠️ Du bist auf main → erzeuge Feature-Branch: $newb"
    git checkout -b "$newb"
    BRANCH="$newb"; UPSTREAM="origin/$BRANCH"
  fi

  # add/commit (Auto-Message)
  git add -A
  if git diff --staged --quiet; then
    log "ℹ️ Keine neuen Änderungen zu committen."
  else
    local msg; msg="wg: mobile push $(date -u "+%Y-%m-%d %H:%M:%S UTC") on ${BRANCH}"
    git commit -m "$msg" || die "Commit fehlgeschlagen"
    log "✅ Commit: $msg"
  fi

  # rebase-pull → bei Konflikt interaktiv entscheiden
  log "🔄 Pull --rebase …"
  if ! git pull --rebase -s recursive -X ours; then
    echo "⚠️ Rebase-Konflikt. Optionen:"
    echo "   1) lokal bevorzugen  (-X ours)"
    echo "   2) remote bevorzugen (-X theirs)"
    echo "   3) rebase neu versuchen"
    echo "   4) ff-only versuchen"
    echo "   5) abbrechen"
    read -p "➤ Auswahl [1/2/3/4/5]: " sel
    case "$sel" in
      1) git rebase --abort >/dev/null 2>&1 || true; git pull -s recursive -X ours   || die "fehlgeschlagen" ;;
      2) git rebase --abort >/dev/null 2>&1 || true; git pull -s recursive -X theirs || die "fehlgeschlagen" ;;
      3) git rebase --continue 2>/dev/null || { git rebase --abort >/dev/null 2>&1 || true; git pull --rebase || die "fehlgeschlagen"; } ;;
      4) git rebase --abort >/dev/null 2>&1 || true; git pull --ff-only || die "nicht möglich" ;;
      5) log "Abbruch."; return 1 ;;
      *) log "Abbruch."; return 1 ;;
    esac
  fi

  # push
  if git rev-parse --verify -q "refs/remotes/origin/$BRANCH" >/dev/null 2>&1; then
    git push origin "$BRANCH" || die "Push fehlgeschlagen"
  else
    git push -u origin "$BRANCH" || die "Push fehlgeschlagen"
  fi
  log "🚀 Branch gepusht: origin/$BRANCH"

  # PR-Link ausgeben (Auto-Titel)
  cmd_pr
}

# 1 = Bash neu, 2 = Debian-Proot (Bun) – mit --here
cmd_reload(){
  ensure_repo
  local mode="" keep_here=0
  while [ $# -gt 0 ]; do case "$1" in 1|2) mode="$1";; --here|-H) keep_here=1;; esac; shift; done
  local repo_abs cur_abs target
  repo_abs="$(cd "$repo" 2>/dev/null && pwd -P || echo "$repo")"
  cur_abs="$(pwd -P 2>/dev/null || pwd)"
  if [ $keep_here -eq 1 ] && [ "${cur_abs#"$repo_abs"/}" != "$cur_abs" ]; then target="$cur_abs"; else target="$repo_abs"; fi

  if [ -z "$mode" ]; then
    echo "🔄 Shell neu… (Ziel: $target)"
    echo "  1) Bash"
    [ "$is_termux" -eq 1 ] && echo "  2) Debian-Proot (Bun)"
    read -p "➤ Eingabe [1$([ "$is_termux" -eq 1 ] && echo "/2")]: " mode
  fi

  case "$mode" in
    2)
      if [ "$is_termux" -eq 1 ]; then
        echo "🚀 Debian-Proot in: $target"
        exec proot-distro login debian -- bash -lc "cd \"$target\" || cd ~; exec bash -l"
      else
        echo "ℹ️ Option 2 nur auf Termux sinnvoll. Starte normale Shell."
        exec bash -l
      fi
      ;;
    *)
      echo "🚀 Bash in: $target"
      exec bash -lc "cd \"$target\" || cd ~; exec bash -l"
      ;;
  esac
}

usage(){
  cat <<USAGE
⚡ wg usage:
  wg go | doctor
  wg clean [--apply]
  wg heal
  wg pr [Titel optional]        # PR-Link (Auto-Titel, wenn leer)
  wg send                       # PR-only Flow (nie direkt auf main)
  wg reload [1|2] [--here]      # 1=Bash, 2=Proot/Bun (nur Termux)
USAGE
}

case "$1" in
  go) shift; cmd_go "$@" ;;
  doctor) shift; cmd_doctor "$@" ;;
  clean) shift; cmd_clean "$@" ;;
  heal) shift; cmd_heal "$@" ;;
  pr) shift; cmd_pr "$@" ;;
  send) shift; cmd_send "$@" ;;
  reload) shift; cmd_reload "$@" ;;
  ""|help|-h|--help) usage ;;
  *) echo "Unbekannt: $1"; usage; exit 1 ;;
esac
```

### 📄 wg.bak.1757191428

**Größe:** 3.13 KB

```
#!/data/data/com.termux/files/usr/bin/bash
# Weltgewebe CLI (Termux) – interaktiv; KEIN set -e/-u/pipefail

repo="/data/data/com.termux/files/home/weltgewebe-repo"

log(){ printf "%s\n" "$*"; }
ensure_repo(){ [ -d "$repo/.git" ] || { log "❌ Repo nicht gefunden: $repo"; exit 1; }; }

cmd_go(){
  ensure_repo
  log "🌍 Starte Weltgewebe-Checks…"
  ( cd "$repo" && ls -1 .codex scripts scripts/k6 scripts/lighthouse 2>/dev/null )
}

cmd_doctor(){
  ensure_repo
  log "🩺 Doctor:"
  command -v git  >/dev/null && log "✅ git: $(git --version)" || log "❌ git fehlt"
  command -v curl >/dev/null && log "✅ curl: $(curl --version | head -n1)" || log "❌ curl fehlt"
  ( cd "$repo" && log "📜 letzte Commits:" && git --no-pager log --oneline -n 5 )
}

cmd_send(){
  ensure_repo
  cd "$repo" || exit 1
  branch="$(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo main)"
  ts="$(date -u "+%Y-%m-%d %H:%M:%S UTC")"
  host="$(termux-info 2>/dev/null | awk -F": " "/Device model/{print \}" | head -n1)"
  msg="wg: mobile push $ts${host:+ [$host]}"
  git add -A
  if git diff --staged --quiet; then
    log "ℹ️  Keine Änderungen zum Commit. Pushe nur."
  else
    git commit -m "$msg" || { log "❌ Commit fehlgeschlagen"; exit 1; }
  fi
  git push origin "$branch" || { log "❌ Push fehlgeschlagen"; exit 1; }
  log "✅ Gesendet auf $branch"
}

cmd_sync(){
  ensure_repo
  cd "$repo" || exit 1
  git fetch origin
  git pull --rebase -s recursive -X ours origin "$(git rev-parse --abbrev-ref HEAD)"
}

cmd_heal(){
  ensure_repo
  cd "$repo" || exit 1
  branch="$(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo main)"
  backup="backup/${branch}-$(date +%Y%m%d-%H%M%S)"
  log "🛟 Backup-Branch: $backup"
  git branch -c "$backup" || true
  log "⬇️  Hole aktuelle Remote-Daten…"
  git fetch origin
  log "🔄 Versuche Rebase (lokal bevorzugt)…"
  if git pull --rebase -s recursive -X ours origin "$branch"; then
    log "✅ Rebase erfolgreich"; return
  fi
  log "⚠️ Rebase gescheitert – versuche Merge (ours bevorzugt)…"
  if git merge -s recursive -X ours origin/"$branch"; then
    log "✅ Merge erfolgreich"; return
  fi
  log "❌ Automatisches Heilen fehlgeschlagen. Bitte manuell lösen."
  exit 1
}

cmd_reload(){
  echo "🔄 Shell wird neu gestartet..."
  echo "Wähle Ziel:"
  echo "  1) Termux (Standard)"
  echo "  2) Debian-Proot (für Bun)"
  read -p "➤ Eingabe [1/2]: " choice
  case "$choice" in
    2) echo "🚀 Starte Debian-Shell..."; exec proot-distro login debian -- bash -l ;;
    *) echo "🚀 Starte Termux-Shell neu..."; exec bash -l ;;
  esac
}

usage(){
  cat <<USAGE
⚡ wg usage:
  wg go       - schnelle Checks/Startprobe
  wg doctor   - Umgebung prüfen (git, curl, bun)
  wg send     - add/commit (Auto-Msg) + push
  wg sync     - fetch + pull --rebase (ours)
  wg heal     - Konflikte heilen (Backup → rebase -X ours → merge -X ours)
  wg reload   - Shell neu starten (Termux/Proot)
USAGE
}

case "$1" in
  go)      cmd_go ;;
  doctor)  cmd_doctor ;;
  send)    cmd_send ;;
  sync)    cmd_sync ;;
  heal)    cmd_heal ;;
  reload)  cmd_reload ;;
  ""|help|-h|--help) usage ;;
  *) echo "Unbekannt: $1"; usage; exit 1 ;;
esac
```

### 📄 wg.bak.1757191456

**Größe:** 3.13 KB

```
#!/data/data/com.termux/files/usr/bin/bash
# Weltgewebe CLI (Termux) – interaktiv; KEIN set -e/-u/pipefail

repo="/data/data/com.termux/files/home/weltgewebe-repo"

log(){ printf "%s\n" "$*"; }
ensure_repo(){ [ -d "$repo/.git" ] || { log "❌ Repo nicht gefunden: $repo"; exit 1; }; }

cmd_go(){
  ensure_repo
  log "🌍 Starte Weltgewebe-Checks…"
  ( cd "$repo" && ls -1 .codex scripts scripts/k6 scripts/lighthouse 2>/dev/null )
}

cmd_doctor(){
  ensure_repo
  log "🩺 Doctor:"
  command -v git  >/dev/null && log "✅ git: $(git --version)" || log "❌ git fehlt"
  command -v curl >/dev/null && log "✅ curl: $(curl --version | head -n1)" || log "❌ curl fehlt"
  ( cd "$repo" && log "📜 letzte Commits:" && git --no-pager log --oneline -n 5 )
}

cmd_send(){
  ensure_repo
  cd "$repo" || exit 1
  branch="$(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo main)"
  ts="$(date -u "+%Y-%m-%d %H:%M:%S UTC")"
  host="$(termux-info 2>/dev/null | awk -F": " "/Device model/{print \}" | head -n1)"
  msg="wg: mobile push $ts${host:+ [$host]}"
  git add -A
  if git diff --staged --quiet; then
    log "ℹ️  Keine Änderungen zum Commit. Pushe nur."
  else
    git commit -m "$msg" || { log "❌ Commit fehlgeschlagen"; exit 1; }
  fi
  git push origin "$branch" || { log "❌ Push fehlgeschlagen"; exit 1; }
  log "✅ Gesendet auf $branch"
}

cmd_sync(){
  ensure_repo
  cd "$repo" || exit 1
  git fetch origin
  git pull --rebase -s recursive -X ours origin "$(git rev-parse --abbrev-ref HEAD)"
}

cmd_heal(){
  ensure_repo
  cd "$repo" || exit 1
  branch="$(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo main)"
  backup="backup/${branch}-$(date +%Y%m%d-%H%M%S)"
  log "🛟 Backup-Branch: $backup"
  git branch -c "$backup" || true
  log "⬇️  Hole aktuelle Remote-Daten…"
  git fetch origin
  log "🔄 Versuche Rebase (lokal bevorzugt)…"
  if git pull --rebase -s recursive -X ours origin "$branch"; then
    log "✅ Rebase erfolgreich"; return
  fi
  log "⚠️ Rebase gescheitert – versuche Merge (ours bevorzugt)…"
  if git merge -s recursive -X ours origin/"$branch"; then
    log "✅ Merge erfolgreich"; return
  fi
  log "❌ Automatisches Heilen fehlgeschlagen. Bitte manuell lösen."
  exit 1
}

cmd_reload(){
  echo "🔄 Shell wird neu gestartet..."
  echo "Wähle Ziel:"
  echo "  1) Termux (Standard)"
  echo "  2) Debian-Proot (für Bun)"
  read -p "➤ Eingabe [1/2]: " choice
  case "$choice" in
    2) echo "🚀 Starte Debian-Shell..."; exec proot-distro login debian -- bash -l ;;
    *) echo "🚀 Starte Termux-Shell neu..."; exec bash -l ;;
  esac
}

usage(){
  cat <<USAGE
⚡ wg usage:
  wg go       - schnelle Checks/Startprobe
  wg doctor   - Umgebung prüfen (git, curl, bun)
  wg send     - add/commit (Auto-Msg) + push
  wg sync     - fetch + pull --rebase (ours)
  wg heal     - Konflikte heilen (Backup → rebase -X ours → merge -X ours)
  wg reload   - Shell neu starten (Termux/Proot)
USAGE
}

case "$1" in
  go)      cmd_go ;;
  doctor)  cmd_doctor ;;
  send)    cmd_send ;;
  sync)    cmd_sync ;;
  heal)    cmd_heal ;;
  reload)  cmd_reload ;;
  ""|help|-h|--help) usage ;;
  *) echo "Unbekannt: $1"; usage; exit 1 ;;
esac
```

### 📄 wg.bak.1757191553

**Größe:** 3.13 KB

```
#!/data/data/com.termux/files/usr/bin/bash
# Weltgewebe CLI (Termux) – interaktiv; KEIN set -e/-u/pipefail

repo="/data/data/com.termux/files/home/weltgewebe-repo"

log(){ printf "%s\n" "$*"; }
ensure_repo(){ [ -d "$repo/.git" ] || { log "❌ Repo nicht gefunden: $repo"; exit 1; }; }

cmd_go(){
  ensure_repo
  log "🌍 Starte Weltgewebe-Checks…"
  ( cd "$repo" && ls -1 .codex scripts scripts/k6 scripts/lighthouse 2>/dev/null )
}

cmd_doctor(){
  ensure_repo
  log "🩺 Doctor:"
  command -v git  >/dev/null && log "✅ git: $(git --version)" || log "❌ git fehlt"
  command -v curl >/dev/null && log "✅ curl: $(curl --version | head -n1)" || log "❌ curl fehlt"
  ( cd "$repo" && log "📜 letzte Commits:" && git --no-pager log --oneline -n 5 )
}

cmd_send(){
  ensure_repo
  cd "$repo" || exit 1
  branch="$(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo main)"
  ts="$(date -u "+%Y-%m-%d %H:%M:%S UTC")"
  host="$(termux-info 2>/dev/null | awk -F": " "/Device model/{print \}" | head -n1)"
  msg="wg: mobile push $ts${host:+ [$host]}"
  git add -A
  if git diff --staged --quiet; then
    log "ℹ️  Keine Änderungen zum Commit. Pushe nur."
  else
    git commit -m "$msg" || { log "❌ Commit fehlgeschlagen"; exit 1; }
  fi
  git push origin "$branch" || { log "❌ Push fehlgeschlagen"; exit 1; }
  log "✅ Gesendet auf $branch"
}

cmd_sync(){
  ensure_repo
  cd "$repo" || exit 1
  git fetch origin
  git pull --rebase -s recursive -X ours origin "$(git rev-parse --abbrev-ref HEAD)"
}

cmd_heal(){
  ensure_repo
  cd "$repo" || exit 1
  branch="$(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo main)"
  backup="backup/${branch}-$(date +%Y%m%d-%H%M%S)"
  log "🛟 Backup-Branch: $backup"
  git branch -c "$backup" || true
  log "⬇️  Hole aktuelle Remote-Daten…"
  git fetch origin
  log "🔄 Versuche Rebase (lokal bevorzugt)…"
  if git pull --rebase -s recursive -X ours origin "$branch"; then
    log "✅ Rebase erfolgreich"; return
  fi
  log "⚠️ Rebase gescheitert – versuche Merge (ours bevorzugt)…"
  if git merge -s recursive -X ours origin/"$branch"; then
    log "✅ Merge erfolgreich"; return
  fi
  log "❌ Automatisches Heilen fehlgeschlagen. Bitte manuell lösen."
  exit 1
}

cmd_reload(){
  echo "🔄 Shell wird neu gestartet..."
  echo "Wähle Ziel:"
  echo "  1) Termux (Standard)"
  echo "  2) Debian-Proot (für Bun)"
  read -p "➤ Eingabe [1/2]: " choice
  case "$choice" in
    2) echo "🚀 Starte Debian-Shell..."; exec proot-distro login debian -- bash -l ;;
    *) echo "🚀 Starte Termux-Shell neu..."; exec bash -l ;;
  esac
}

usage(){
  cat <<USAGE
⚡ wg usage:
  wg go       - schnelle Checks/Startprobe
  wg doctor   - Umgebung prüfen (git, curl, bun)
  wg send     - add/commit (Auto-Msg) + push
  wg sync     - fetch + pull --rebase (ours)
  wg heal     - Konflikte heilen (Backup → rebase -X ours → merge -X ours)
  wg reload   - Shell neu starten (Termux/Proot)
USAGE
}

case "$1" in
  go)      cmd_go ;;
  doctor)  cmd_doctor ;;
  send)    cmd_send ;;
  sync)    cmd_sync ;;
  heal)    cmd_heal ;;
  reload)  cmd_reload ;;
  ""|help|-h|--help) usage ;;
  *) echo "Unbekannt: $1"; usage; exit 1 ;;
esac
```

### 📄 wg.bak.1757191907

**Größe:** 3.06 KB

```
#!/data/data/com.termux/files/usr/bin/bash
# Weltgewebe CLI (Termux) – interaktiv; KEIN set -e/-u/pipefail

repo_default="$HOME/weltgewebe-repo"
repo_detected="$(git -C "$PWD" rev-parse --show-toplevel 2>/dev/null)"
repo="${repo_detected:-$repo_default}"

log(){ printf "%s\n" "$*"; }
ensure_repo(){ [ -d "$repo/.git" ] || { log "❌ Repo nicht gefunden: $repo"; exit 1; }; }

cmd_go(){
  ensure_repo
  log "🌍 Starte Weltgewebe-Checks…"
  ( cd "$repo" && ls -1 .codex scripts scripts/k6 scripts/lighthouse 2>/dev/null )
  if [ -x "$HOME/debian-bun" ]; then
    log "➡️  Bun (Proot): $(~/debian-bun 2>/dev/null)"
  else
    log "ℹ️  Bun (Proot) nicht eingerichtet – nur Dateichecks."
  fi
}

cmd_doctor(){
  ensure_repo
  log "🩺 Doctor:"
  command -v git >/dev/null && log "✅ git: $(git --version)" || log "❌ git fehlt"
  command -v curl >/dev/null && log "✅ curl: $(curl --version | head -n1)" || log "❌ curl fehlt"
  if [ -x "$HOME/debian-bun" ]; then log "✅ bun (proot): $(~/debian-bun)"; else log "⚠️ bun nicht aktiv (ok)"; fi
  ( cd "$repo" && log "📜 letzte Commits:" && git --no-pager log --oneline -n 5 )
}

cmd_send(){
  ensure_repo
  cd "$repo" || exit 1
  branch="$(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo main)"
  ts="$(date -u "+%Y-%m-%d %H:%M:%S UTC")"
  host="$(termux-info 2>/dev/null | awk -F": " "/Device model/{print \$2}" | head -n1)"
  msg="wg: mobile push $ts${host:+ [$host]}"
  git add -A
  if git diff --staged --quiet; then
    log "ℹ️  Keine Änderungen zum Commit. Pushe nur."
  else
    git commit -m "$msg" || { log "❌ Commit fehlgeschlagen"; exit 1; }
  fi
  git push origin "$branch" || { log "❌ Push fehlgeschlagen"; exit 1; }
  log "✅ Gesendet auf $branch"
}

cmd_sync(){
  ensure_repo
  cd "$repo" || exit 1
  git fetch origin
  git pull --rebase -s recursive -X ours origin "$(git rev-parse --abbrev-ref HEAD)"
}

cmd_reload(){
  # Nutzung: wg reload [1|2]
  local choice="$1"
  local repo_abs; repo_abs="$(cd "$repo" 2>/dev/null && pwd || echo "$repo")"

  if [ -z "$choice" ]; then
    echo "🔄 Shell wird neu gestartet…"
    echo "  1) Termux (Standard)"
    echo "  2) Debian-Proot (für Bun; startet im Repo)"
    read -p "➤ Eingabe [1/2]: " choice
  fi

  case "$choice" in
    2)
      echo "🚀 Starte Debian-Shell (im Repo): $repo_abs"
      exec proot-distro login debian -- bash -lc "cd \"$repo_abs\" 2>/dev/null || cd ~; exec bash -l"
      ;;
    *)
      echo "🚀 Starte Termux-Shell neu…"
      exec bash -l
      ;;
  esac
}

usage(){
  cat <<USAGE
⚡ wg usage:
  wg go        – schnelle Checks/Startprobe
  wg doctor    – Umgebung prüfen (git, curl, bun)
  wg send      – add/commit (auto-msg) + push
  wg sync      – fetch + pull --rebase (lokal bevorzugt)
  wg reload [1|2] – 1: Termux, 2: Debian-Proot IM REPO
USAGE
}

case "$1" in
  go)       shift; cmd_go "$@" ;;
  doctor)   shift; cmd_doctor "$@" ;;
  send)     shift; cmd_send "$@" ;;
  sync)     shift; cmd_sync "$@" ;;
  reload)   shift; cmd_reload "$1" ;;
  ""|help|-h|--help) usage ;;
  *) echo "Unbekannt: $1"; usage; exit 1 ;;
esac
```

### 📄 wg.bak.1757192266

**Größe:** 1.38 KB

```
#!/usr/bin/env bash
# Weltgewebe CLI – interaktiv; KEIN set -e/-u/pipefail (Termux-tolerant)

repo_default="$(cd "$(dirname "$0")" && pwd)"
repo_detected="$(git -C "$PWD" rev-parse --show-toplevel 2>/dev/null)"
repo="${repo_detected:-$repo_default}"

is_termux=0; case "$PREFIX" in
  */com.termux/*) is_termux=1 ;;
esac

log(){ printf "%s\n" "$*"; }
ensure_repo(){ [ -d "$repo/.git" ] || { log "❌ Repo nicht gefunden: $repo"; exit 1; }; }

cmd_go(){
  ensure_repo
  log "🌍 Startchecks…"
  ( cd "$repo" && ls -1 .codex scripts 2>/dev/null || true )
  if [ "$is_termux" -eq 1 ] && [ -x "$HOME/debian-bun" ]; then
    log "➡️  Bun (Proot): $(~/debian-bun 2>/dev/null)"
  else
    log "ℹ️  Bun-Proot optional; auf Codespaces/PC nicht nötig."
  fi
}

cmd_doctor(){
  ensure_repo
  log "🩺 Doctor:"
  command -v git >/dev/null && log "✅ git: $(git --version)" || log "❌ git fehlt"
  command -v curl >/dev/null && log "✅ curl: $(curl --version | head -n1)" || log "❌ curl fehlt"
  if [ "$is_termux" -eq 1 ] && [ -x "$HOME/debian-bun" ]; then log "✅ bun (proot): $(~/debian-bun)"; fi
  ( cd "$repo" && log "📜 letzte Commits:" && git --no-pager log --oneline -n 5 || true )
}

cmd_send(){
  ensure_repo; cd "$repo" || exit 1
  branch="$(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo main)"
  ts="$(date -u "+%Y-%m-%d %H:%M:%S UTC")"
  host="$( (termux-info 2>/dev/null | awk -F:
```

### 📄 wg.bak.1757192687

**Größe:** 2.11 KB

```
#!/usr/bin/env bash
# wg – Weltgewebe CLI (portabel). Kein set -e/-u/pipefail (Termux-tolerant)

repo_default="$(cd "$(dirname "$0")" && pwd)"
repo_detected="$(git -C "$PWD" rev-parse --show-toplevel 2>/dev/null)"
repo="${repo_detected:-$repo_default}"

is_termux=0; case "$PREFIX" in */com.termux/*) is_termux=1 ;; esac

log(){ printf "%s\n" "$*"; }
ensure_repo(){ [ -d "$repo/.git" ] || { log "❌ Repo nicht gefunden: $repo"; exit 1; }; }

cmd_go(){ ensure_repo; log "🌍 Startchecks…"; (cd "$repo" && ls -1 .codex scripts 2>/dev/null || true); }
cmd_doctor(){ ensure_repo; log "🩺 Doctor:"; command -v git >/dev/null && git --version || log "❌ git fehlt"; }
cmd_send(){ ensure_repo; cd "$repo" || exit 1;
  branch="$(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo main)"
  ts="$(date -u "+%Y-%m-%d %H:%M:%S UTC")"
  host="$(hostname 2>/dev/null || true)"; msg="wg: push $ts${host:+ [$host]}"
  git add -A; git diff --staged --quiet || git commit -m "$msg"; git push origin "$branch"; log "✅ Gesendet auf $branch"; }
cmd_sync(){ ensure_repo; cd "$repo" || exit 1; git fetch origin; git pull --rebase -s recursive -X ours origin "$(git rev-parse --abbrev-ref HEAD)"; }
cmd_reload(){ # wg reload [1|2]
  local choice="$1" repo_abs; repo_abs="$(cd "$repo" 2>/dev/null && pwd || echo "$repo")"
  if [ -z "$choice" ]; then
    echo "🔄 Shell neu…"; echo "  1) Shell neu starten"; [ "$is_termux" -eq 1 ] && echo "  2) Debian-Proot (im Repo)"
    read -p "➤ Eingabe [1$([ "$is_termux" -eq 1 ] && echo "/2")]: " choice
  fi
  case "$choice" in
    2) if [ "$is_termux" -eq 1 ]; then echo "🚀 Debian-Proot im Repo: $repo_abs"; exec proot-distro login debian -- bash -lc "cd \"$repo_abs\" || cd ~; exec bash -l"; else exec bash -l; fi ;;
    *) echo "🚀 Starte Shell neu…"; exec bash -l ;;
  esac
}

usage(){ cat <<USAGE
⚡ wg usage:
  wg go | doctor | send | sync | reload [1|2]
USAGE
}

case "$1" in
  go) shift; cmd_go "$@";;
  doctor) shift; cmd_doctor "$@";;
  send) shift; cmd_send "$@";;
  sync) shift; cmd_sync "$@";;
  reload) shift; cmd_reload "$1";;
  ""|help|-h|--help) usage;;
  *) echo "Unbekannt: $1"; usage; exit 1;;
esac
```

