Alles klar — hier ist der Masterplan v9.3 (final), maximal detailliert, community‑first, small‑team‑tauglich, mit JetStream als unverrückbarem Event‑Backbone. Er integriert die offenen Punkte aus v8.3/v9.1/v9.2: stärkere Federation‑Details & Gates, klarere Hash‑Validierung pro Aggregat, getrennte Worker, schlankere Timeline (tägliche Snapshots, Delta per Flag), granularer WebPush (opt‑in), harte Ops‑Runbooks mit MTTR‑Zielen und automatisierte Kostenmetriken (<0,01 €/1k Events).

⸻

0. Leitplanken (fix)
	•	Alles ist Event (unveränderlich, auditierbar; Hash‑Chain pro Aggregat)
	•	Radikale Transparenz (DLQ sichtbar, Admin‑Aktionen sind Events)
	•	Small‑Team‑Ops (1–2 Personen; MTTR < 60 min; niedrige Fixkosten)
	•	Kostenmetriken (< 0,01 € / 1 000 Events; in Grafana sichtbar)
	•	Autonomie & Ethik (Peer‑Verifizierung; Datenschutz by design)
	•	JetStream bleibt (NATS JetStream = SoR + Replay); Redis nur Fallback

⸻

1. Nutzererlebnis (Phase‑A‑vollständig)

1.1 Karte & Slider (persistente UI)
	•	Mitte: MapLibre‑Karte. Knoten (Marker), Fäden (Linien), Garn (verzwirnt, permanent). Fäden verblassen über 7 Tage (Opacity‑Decay).
	•	Links – „Webrat“: Governance & Anträge nahe des Kartenausschnitts; Abstimmen/Delegieren; Moderationsfäden.
	•	Rechts – „Stübchen“: Threads/Chats der im aktuellen Viewport sichtbaren Knoten (dynamisch gebunden an Karte).
	•	Oben – „Konto“: Eigenes Profil, Delegationsstatus, Benachrichtigungen (Opt‑ins), Export/Löschung, Geräte.

1.2 Räume/Fenster
	•	Jeder Knoten öffnet einen Raum (Panel/Modal mit Tabs: Thread, Teilnehmende, Gestaltungsbereich).
	•	Räume sind live‑synchron (WS); lokaler Editor mit Outbox (Dexie) & Retry.

1.3 Suche (optional in A; Flag)
	•	Fuzzy‑Suche über knoten.title, antraege.title, rollen.name.
	•	Treffer → flyTo() + Öffnen des Raums.
	•	FF_SEARCH=true (aktivierbar; default true).

1.4 Onboarding‑Tour (optional)
	•	5 Schritte (Karte → Knoten → Faden → Raum → Delegation).
	•	FF_TOUR=false (default aus, aktivierbar).

1.5 Timeline (Phase‑A schlank)
	•	Daily Snapshot View (Kalenderleiste unten).
	•	Delta‑Replay (SSE) per Flag (siehe §8). Default aus in A.

1.6 WebPush (opt‑in)
	•	Opt‑in im Konto, granular:
	•	Neue Anträge in meiner Weberei
	•	Erwähnungen in Threads
	•	Änderungen an meinen Knoten/Fäden
	•	Aktionen meines Delegierten
	•	Default aus in A: FF_PUSH=false, vorbereitet (siehe §10).

⸻

2. Domänenmodell & Events

2.1 Kerntabellen (Projektionen; rekonstruierbar)
	•	rollen(id UUID PK, name TEXT, h3 BIGINT, verified_by UUID[], verified_at, created_at)
	•	knoten(id UUID PK, typ ENUM, title TEXT, content JSONB, h3 BIGINT, created_at, expires_at, is_yarn BOOL)
	•	faeden(id UUID PK, from_rolle UUID, to_knoten UUID, typ ENUM, created_at, expires_at, is_yarn BOOL)
	•	antraege(id UUID PK, weavery_h3 BIGINT, title TEXT, content JSONB, status ENUM, created_at, decision_deadline)
	•	stimmen(id UUID, antrag_id, rolle_id, vote ENUM, delegated_from UUID NULL, created_at)
	•	delegationen(id UUID, from_rolle, to_rolle, valid_until, created_at)
	•	zahlungen(id UUID, rolle_id, betrag_cents INT, status ENUM, source ENUM, created_at, settled_at)
	•	Indizes: siehe §5.2.

2.2 Enums (Beispiele)
	•	knoten_typ: ideen, event, ressource
	•	faden_typ: interesse, beitrag, mitarbeit
	•	antrag_status: offen, angenommen, abgelehnt, quorum_verfehlt
	•	vote: ja, nein, enthaltung
	•	zahlung_status: initiated, pending, settled, failed, reversed
	•	source: sepa, stripe

2.3 Eventschema (immutable; Hash‑Chain pro Aggregat)

{
  "id": "ulid",
  "type": "KnotenGeknüpft|FadenGesponnen|...|AdminActionExecuted",
  "aggregateId": "uuid",               // z.B. knoten-uuid
  "aggregateType": "knoten|faden|antrag|rolle|zahlung",
  "prev_hash": "hex|null",             // Hash des letzten Events DIESES Aggregats
  "ts": "RFC3339",
  "subject": "wg.<ort>.<domain>",      // z.B. wg.kleinroennau.governance
  "payload": { "...": "..." },         // zod-validiert
  "meta": {
    "actor": "rolle-uuid",
    "ip_hash": "sha256",
    "ua_hash": "sha256",
    "msg_id": "UUID|ulid"             // für JetStream/Redis dedup
  }
}

Wichtige Eventtypen (Auszug)
	•	KnotenGeknüpft, KnotenVerfallen, KnotenVerzwirnt
	•	FadenGesponnen, FadenVerfallen, FadenVerzwirnt
	•	AntragGestellt, EinspruchEingegangen, StimmeAbgegeben, DelegationErstellt, DelegationWiderrufen, DelegationAbgelaufen, QuorumErreicht, AntragEntschieden
	•	RolleVerifiziert (2‑von‑N Peer‑Bestätigung)
	•	SpendeInitiated|Pending|Settled|Failed|Reversed
	•	EventRejected (Validierung), DLQEvent (Dead‑Letter)
	•	AdminActionExecuted (z. B. CSV‑Bestätigung)
	•	KostenAktualisiert (Monatskosten in EUR; für Metriken)
	•	PushSubscribed|PushUnsubscribed|PushBenachrichtigungGesendet

⸻

3. Event‑Backbone (NATS JetStream) & Fallback

3.1 Streams & Subjects
	•	Streams pro Ort: wg_<ort>_social (wg..social.*), wg_<ort>_governance, wg_<ort>_finance, wg_<ort>_dlq
	•	Policies: max_age=336h (14 Tage), max_bytes=20GB, storage=file, replicas=3 (B), Phase‑A replicas=1
	•	Dedup: duplicate_window=2m (Nats‑Msg‑Id)

3.2 Consumers
	•	projection_worker: pull, ack_wait=60s, max_ack_pending=1000
	•	ws_fanout: pull, max_ack_pending=2000, batching tick=1.5 s
	•	dlq_worker: pull, deliver=all

3.3 Fallback (Redis Streams, automatisch)
	•	Adapter versucht jetstream.publish(), bei Fehler → redis.xadd() + FallbackActivated‑Event.
	•	Redis‑Dedup: SETEX dedupe:<msgId> 120 1 vor XADD.
	•	Replay‑API konsumiert immer JetStream (wenn verfügbar).

⸻

4. Hash‑Validierung (pro Aggregat)
	•	Beim Publish: prev_hash = SHA‑256(serialized last event of aggregateId).
	•	Beim Consume (Worker):
	1.	Lade expected_prev (aus lokaler Aggregat‑Map oder PG‑cache).
	2.	if evt.prev_hash != expected_prev → publish EventRejected to DLQ (mit reason).
	•	Genesis‑Events (z. B. KnotenGeknüpft) setzen prev_hash=null|GENESIS.

⸻

5. Datenbank (PostgreSQL + PostGIS)

5.1 Storage
	•	PostGIS für Geometrie‑Wahrheit; H3 BIGINT als Hot‑Path‑Index.
	•	events_archive (append‑only, parti­tioned by day/month), UNIQUE(id).

5.2 Indizes (Auszug)

CREATE INDEX ON knoten (h3, created_at);
CREATE INDEX ON faeden (to_knoten, expires_at);
CREATE INDEX ON stimmen (antrag_id, created_at);
CREATE UNIQUE INDEX ON stimmen (antrag_id, rolle_id); -- idempotent
CREATE INDEX ON delegationen (to_rolle, valid_until);
CREATE INDEX ON antraege (weavery_h3, status);

5.3 Partitionierung
	•	events_archive: RANGE(ts) monatlich; pg_partman (Phase B) oder Hand‑DDL (A).

⸻

6. Backend (Fastify API) & Validierung
	•	Type‑safe: Monorepo (pnpm), gemeinsames Paket @wg/schemas (Zod) für API & Frontend.
	•	Endpunkte (Auszug):
	•	POST /api/knoten → validiert (Zod), publish Event, 201
	•	POST /api/abstimmen → rejects wenn Dupe (UNIQUE) → EventRejected
	•	GET /api/replay?day=YYYY‑MM‑DD → Snapshot + (optional) Delta‑SSE (Flag)
	•	GET /health/live (nur Prozess), GET /health/ready (PG+NATS+Redis)
	•	Fehler: Jede abgewiesene Aktion → EventRejected ins DLQ.

⸻

7. WS‑Fanout (Echtzeit)
	•	Gruppen nach H3/Quadkey des Viewports; Map hält tile→clientSet.
	•	Tick 1.5 s: sende Diffs seit lastTs je Tile; Heartbeat alle 30 s.
	•	Backpressure: langsamem Client → per‑Client Queue gecappt; Drop + Hinweis.

⸻

8. Timeline (schlank in A)
	•	Daily Snapshots (Worker schreibt state_<YYYYMMDD>.json.gz in PG/Objekt‑Store).
	•	Delta‑SSE optional: FF_TIMELINE_DELTA=false (A‑default).
	•	Laufzeit‑Gate (wenn aktiv): „Replay 500 Events in ≤ 300 ms P95“.

⸻

9. Timer & Jobs (BullMQ + Redis)
	•	Verfall 7 Tage: repeatable Job; Missed‑Scanner beim Start (idempotent).
	•	Delegation: Ablauf überwachen → DelegationAbgelaufen publish.
	•	FinTS Polling: s. §12; Backoff (30→60→120 min), idempotent (jobId=msgId).

⸻

10. WebPush (opt‑in)
	•	Service Worker registriert; Abos als Events:
	•	PushSubscribed {rolle_id, channels[]}
	•	PushBenachrichtigungGesendet {reason, target_count}
	•	FF_PUSH=false in A; Phase‑B Gate: Push‑Latenz ≤ 500 ms P95.

⸻

11. Moderation & Peer‑Verifizierung
	•	RolleVerifiziert: benötigt 2‑von‑N Peer‑Confirmations (IDs verified_by[]).
	•	FlagRaised/FlagResolved: Moderationsfäden; keine Scores.
	•	QuorumErreicht: Mindestteilnahme (z. B. 10 % aktiver Rollen) als Event; sonst quorum_verfehlt.

⸻

12. Zahlungen (Goldfäden)
	•	Zustandsmaschine: initiated → pending → settled|failed|reversed
	•	SEPA/FinTS (A): aqbanking‑CLI via jobs‑Worker; idempotent (msgId = txid).
	•	CSV‑Fallback: Admin‑UI → AdminActionExecuted + SpendeSettled.
	•	Stripe (optional B): Feature‑Flag FF_STRIPE=false (default).

⸻

13. Federation (Phase‑C vorbereitet, Details & Gates)

13.1 Leafnodes
	•	Ports: client 4222, cluster 6222, leafnodes 7422 (TLS, mTLS).
	•	DNS‑Discovery: _nats-leaf._tcp.<domain> SRV → host:7422.

13.2 Mirror‑Streams
	•	mirror { name: wg_<ort>_governance ; filters: ["wg.<ort>.governance.>"] }
	•	Signaturen: ed25519 (per‑site keypair), Signatur im Nats-Msg-Id‑header‑suffix.
	•	Revocation: KeyRevoked-Event → Mirrors invalidieren, CRL‑ähnlicher Cache.

13.3 Gates (Phase‑C)
	•	Federation‑Test: „Mirror 1 000 Events cross‑Ort mit Lag < 1 s (P95)“
	•	Backpressure‑Gate: „Leaf disconnect & auto‑heal < 30 s“
	•	Security‑Gate: „Invalid Signature → reject & Alert in ≤ 1 s“

⸻

14. Sicherheit
	•	NGINX: TLS 1.3 only; strikte CSP (nonce, kein unsafe-inline); getrennte Rate‑Limits (API vs WS).
	•	PoW bei Last: Hashcash 20 Bits im WS‑Handshake (Lua/Module) >80 % burst.
	•	fail2ban auf 429/401; Kernel‑SYN‑cookies; minimal privileges (rootless where möglich).
	•	DSGVO: Pseudonymisierung (ip/ua hashes), UserDeleted → Pseudonym‑Alias beibehalten für Audit.

⸻

15. Observability & Kosten

15.1 Prometheus/Grafana
	•	Exporter: node, postgres, nats, redis, app custom.
	•	Kosten‑Metrik:
	•	Gauge: wg_costs_month_eur via KostenAktualisiert‑Event (Admin‑UI).
	•	Counter: wg_events_total{type=...}
	•	PromQL: Kosten / 1 000 Events (30 Tage)

(avg_over_time(wg_costs_month_eur[30d]))
/ ( sum(increase(wg_events_total[30d])) / 1000 )


	•	SLOs:
	•	API P95 ≤ 250 ms
	•	WS Fanout P95 ≤ 300 ms
	•	JetStream Lag P95 < 1 s
	•	Event‑Verlust = 0 (DLQ == 0/24h)
	•	Kosten < 0,01 €/1 000 Events

⸻

16. Runbooks (Auszug; MTTR‑Ziele)

16.1 NATS down (MTTR < 15 min)
	1.	nats server report / nats stream report — Ursache?
	2.	Fallback prüfen: sind FallbackActivated‑Events da?
	3.	Speicher voll → nats stream edit wg_<ort>_* --max_bytes +10G (temporär) ODER --max_age 240h
	4.	Dienste neu starten (docker compose restart n1 api worker-*)
	5.	Replay: nats stream info wg_<ort>_governance --json → Offsets sichern
	6.	Post‑Mortem‑Event: AdminActionExecuted{type:"NATSIncident", root_cause, mttr}

16.2 Consumer‑Lag > 1 s (MTTR < 10 min)
	1.	Grafana: jsm_consumer_lag; PG slow queries?
	2.	docker compose up -d --scale worker-projection=3
	3.	ack_wait=90s erhöhen falls nötig
	4.	DLQ inspizieren; fehlerhafte Events fixen → Re‑publish
	5.	Alert schliessen mit Event

16.3 FinTS Throttle (MTTR < 30 min)
	1.	Backoff anpassen (60→120 min), Status UI‑Hint setzen
	2.	CSV‑Fallback aktivieren (Admin‑UI)
	3.	Bank‑Limits dokumentieren (Event)

(Alle Runbooks als RUNBOOK.md im Repo; Änderungen → RunbookAktualisiert‑Event)

⸻

17. CI/CD & Tests
	•	GitHub Actions
	•	Lint/Type‑check/Unit
	•	Build images (api, worker‑projection, worker‑jobs, web)
	•	Security scans (npm audit, trivy)
	•	Deploy via SSH/FluxCD (GitOps bevorzugt) — Rollback‑Job vorhanden
	•	Tests
	•	k6: 10 k Events/min → Lag‑Gate
	•	Playwright: Onboarding‑Tour (wenn FF_TOUR=true), Suche, Räume
	•	WS Test: 200 Clients, p95<300 ms

⸻

18. Phasen & Gates

Phase A (0–6 Wo): Komplette lokale Weberei
	•	Features: Karte+Slider, Räume, 1:1‑Delegation, 7‑Tage‑Verfall, Daily Timeline, SEPA/FinTS (basic), Moderation, Peer‑Verifizierung (2‑von‑N), Suche (an), Tour (aus), Push (aus).
	•	Infra: 1× NATS (replicas=1), PG+PostGIS, Redis, 2 Worker (projection/jobs), NGINX
	•	Gates
	•	200 gleichzeitige WS‑Clients, p95 ≤ 300 ms
	•	10 k Events/min → Lag < 1 s (synthetisch)
	•	Kosten < 0,01 €/1 000 Events
	•	DLQ=0/24h; MTTR‑Runbooks geübt

Phase B (7–12 Wo): Resilienz & Komfort
	•	NATS‑Cluster (3 Nodes auf getrennten Hosts/DC), PG Partitionierung automatisiert, FF_TIMELINE_DELTA=true, FF_PUSH=true, optional Stripe, Onboarding‑Tour (Flag).
	•	Gates
	•	Replay 500 Events ≤ 300 ms p95
	•	Push‑Latenz ≤ 500 ms p95
	•	Worker Autoscale‑Policy (Lag‑basiert)

Phase C (13–20 Wo): Federation & Skalierung
	•	Leafnodes 7422 (mTLS), Mirror‑Streams (ed25519), DNS‑SRV, KeyRevoked‑Flow.
	•	Gates
	•	Mirror 1 000 Events cross‑Ort mit Lag < 1 s p95
	•	Leaf reconnect & heal < 30 s
	•	Invalid Signature → reject & alert in ≤ 1 s

⸻

19. Kosten (realistisch, Hetzner; A→C)
	•	Phase A (~25–35 €/Monat)
	•	CX31 (API+NATS+Redis) ~10 €
	•	CX21 (PG+PostGIS) ~6 €
	•	StorageBox/Backups ~5 €
	•	Domain/Misc ~5–10 €
	•	Optimierung: Spot‑VMs für worker‑projection (–20 %)
	•	Phase B: + zwei CX11 für NATS‑Cluster (~10–12 €)
	•	Phase C: + Leafnode / Mirror je Region (~10–20 €)

⸻

20. Feature‑Flags (Defaults)

FF_SEARCH=true
FF_TOUR=false
FF_TIMELINE_DELTA=false
FF_PUSH=false
FF_TRANSITIVE_DELEGATION=false
FF_STRIPE=false


⸻

21. Docker‑Compose (gekürzt, mit getrennten Workern & Healthchecks)

services:
  api:
    image: ghcr.io/wg/api:latest
    env_file: [.env]
    ports: ["3000:3000"]
    depends_on: [pg, n1, redis]
    healthcheck: { test: ["CMD", "curl", "-f", "http://localhost:3000/health/ready"], interval: 10s, retries: 5 }

  worker-projection:
    image: ghcr.io/wg/worker:latest
    command: ["node","dist/projection.js"]
    env_file: [.env]
    depends_on: [pg, n1, redis]

  worker-jobs:
    image: ghcr.io/wg/worker:latest
    command: ["node","dist/jobs.js"]
    env_file: [.env]
    depends_on: [pg, n1, redis]

  n1:
    image: nats:2.10-alpine
    command: ["-js","-c","/etc/nats/nats-server.conf"]
    volumes: ["./nats:/etc/nats","nats-data:/data"]
    ports: ["4222:4222","7422:7422"]
    healthcheck: { test: ["CMD","nats","server","report"], interval: 15s, retries: 5 }

  pg:
    image: postgis/postgis:15-3.3
    environment: { POSTGRES_PASSWORD: ${PG_PASSWORD} }
    volumes: ["pg-data:/var/lib/postgresql/data"]
    healthcheck: { test: ["CMD","pg_isready","-U","postgres"], interval: 10s, retries: 5 }

  redis:
    image: redis:7-alpine
    healthcheck: { test: ["CMD","redis-cli","ping"], interval: 10s, retries: 5 }

  nginx:
    image: nginx:stable-alpine
    volumes: ["./nginx.conf:/etc/nginx/nginx.conf:ro"]
    ports: ["80:80","443:443"]
    depends_on: [api]

volumes: { nats-data: {}, pg-data: {} }


⸻

22. Implementierungs‑Skeletons (kritische Stellen)

22.1 Publish mit Hash‑Kette & Fallback

// eventBus.ts (gekürzt)
export async function publish(evt: Event): Promise<void> {
  const last = await aggCache.get(evt.aggregateId);        // {hash: string|null}
  const expected = last?.hash ?? null;
  if (evt.prev_hash !== expected) {
    return publishDLQ({type:"EventRejected", reason:"invalid_prev_hash", evt});
  }
  try {
    await jsPublish(evt.subject, evt, { headers: { "Nats-Msg-Id": evt.meta.msg_id }});
  } catch (e) {
    await redisDedup(evt.meta.msg_id);
    await redisXAdd(evt.subject, evt);
    await jsPublish("wg.system.fallback", { type:"FallbackActivated", evtId: evt.id, reason: String(e) });
  }
  aggCache.set(evt.aggregateId, { hash: sha256(JSON.stringify(evt)) });
}

22.2 Timer‑Missed‑Scanner

// jobs/missed.ts
const now = new Date();
const expired = await db.query(`SELECT id FROM knoten WHERE expires_at < $1 AND is_yarn = false`, [now]);
for (const row of expired.rows) {
  await publish(makeEvt("KnotenVerfallen", row.id));
}

22.3 WS‑Tick (Diff pro Tile)

setInterval(async () => {
  for (const tile of tiles) {
    const delta = await getDeltaSince(tile.id, tile.lastTs);
    if (!delta.length) continue;
    for (const client of tile.clients) {
      if (client.buffered > MAX_QUEUE) { client.close(1008,"slow"); continue; }
      client.send(JSON.stringify({type:"delta", tile: tile.id, events: delta}));
    }
    tile.lastTs = Date.now();
  }
}, 1500);


⸻

23. Test‑ & Gate‑Skripte (Beispiele)
	•	k6 (Events/min):

// pseudo: publish 10k/min, check lag metrics via /metrics scrape

	•	Playwright:
	•	Tour (wenn FF_TOUR=true), Räume öffnen, Abstimmen (UNIQUE‑Constraint), Delegation widerrufen.
	•	WS: 200 Clients; messen p95 Roundtrip.

⸻

24. ASCII‑Übersicht

[SvelteKit PWA + MapLibre + Dexie]
      │  WS (1.5s batch) / HTTPS
      ▼
[Fastify API] ──→ [NATS JetStream] ←─[worker‑projection]
      │             ▲      │          [DLQ worker]
      │             │      └─ mirror/leafnodes (7422, TLS, ed25519)
      ├─→ [Redis (cache, BullMQ)] ←─[worker‑jobs (timers, FinTS)]
      └─→ [PostgreSQL + PostGIS + H3]  (projections, snapshots)
      │
 [NGINX TLS1.3 + RateLimit/PoW]
      │
 [Prometheus/Grafana]  ← metrics + KostenAktualisiert

Legend:
	•	SoR: JetStream (14 Tage; Replay; Dedup)
	•	Speed Layer: H3‑Agg in Memory/Redis → WS
	•	Warm: PG Projections & Snapshots → Timeline
	•	Fallback: Redis Streams auto bei JetStream‑Fehler

⸻

Kurzfazit
	•	Phase‑A liefert die komplette lokale Weberei‑Experience (Räume, Slider, Delegation, Daily‑Timeline) ohne unnötige Enterprise‑Last.
	•	JetStream bleibt Kern (Audit, Replay), Redis nur Fallback.
	•	Ops: harte Runbooks mit MTTR‑Zielen, getrennte Worker, Health‑Checks, Kosten‑Metriken.
	•	Federation ist präzise vorbereitet (Leafnodes 7422, ed25519‑Mirror, Revocation, Gates).
	•	Kosten bleiben niedrig, SLOs messbar, Ethik/Transparenz konsequent.

